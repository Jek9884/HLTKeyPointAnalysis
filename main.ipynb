{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storagenfs/m.melerocavallo/HLTKeyPointAnalysis/venv2/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_metric_learning import losses\n",
    "import data_handler\n",
    "from siamese_network import SiameseNetwork, train\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "from custom_losses import ContrastiveLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch cuda available: True\n"
     ]
    }
   ],
   "source": [
    "torch.zeros(1).cuda()\n",
    "#print(f\"torch version: {torch.__version__}\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "print(f\"torch cuda available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data, _ = data_handler.load(path=\"dataset/\", filename_train=\"train.csv\", sep_char='#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero_train:  13100\n",
      "one_train:  3408\n",
      "zero_val:  3275\n",
      "one_val:  852\n"
     ]
    }
   ],
   "source": [
    "df_train, df_val = data_handler.split_train_data(df_data, perc_split=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = data_handler.concatenate_topics(df_train)\n",
    "df_val = data_handler.concatenate_topics(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/storagenfs/m.melerocavallo/HLTKeyPointAnalysis/venv2/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenized = data_handler.tokenize_df(df_train[:100], BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SiameseNetwork(bert_type=BertModel.from_pretrained('bert-base-uncased'))\n",
    "\n",
    "train_loader = DataLoader(tokenized, shuffle=False, batch_size=32)\n",
    "\n",
    "#train_loss = ContrastiveLoss()\n",
    "train_loss = losses.ContrastiveLoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "# Batch size: 16, 32\n",
    "# Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
    "# Number of epochs: 2, 3, 4\n",
    "\n",
    "# The BERT authors recommend between 2 and 4.\n",
    "epochs = 1\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 batch: 0 / 100 loss: -16.554628372192383\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    encoding = train(model, None, train_loader, ContrastiveLoss, optimizer, epoch, scheduler)\n",
    "    #test(model, device, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = ContrastiveLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss\n",
    "    Takes embeddings of two samples and a target label == 1 if samples are from the same class and label == 0 otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.eps = 1e-9\n",
    "\n",
    "    def forward(self, output1, output2, target, size_average=True):\n",
    "        distances = (output2 - output1).pow(2).sum(1)  # squared distances\n",
    "        losses = 0.5 * (torch.matmul(labels.float(), d) +\n",
    "                        torch.matmul((1 + -1 * target).float(), torch.nn.functional.relu(self.margin - (distances + self.eps).sqrt()).pow(2)))\n",
    "        return losses.mean() if size_average else losses.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive = ContrastiveLoss(margin=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = contrastive(output1, output2, labels)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_contrastive_loss(left_feature, right_feature, label, margin):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the contrastive loss as in\n",
    "\n",
    "\n",
    "    L = 0.5 * Y * D^2 + 0.5 * (Y-1) * {max(0, margin - D)}^2\n",
    "\n",
    "    **Parameters**\n",
    "     left_feature: First element of the pair\n",
    "     right_feature: Second element of the pair\n",
    "     label: Label of the pair (0 or 1)\n",
    "     margin: Contrastive margin\n",
    "\n",
    "    **Returns**\n",
    "     Return the loss operation\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    label = label.float()\n",
    "    one = 1.0\n",
    "\n",
    "    d = compute_euclidean_distance(left_feature, right_feature)\n",
    "    d_sqrt = torch.sqrt(compute_euclidean_distance(left_feature, right_feature))\n",
    "    first_part = torch.matmul(one-label, d)# (Y-1)*(d)\n",
    "\n",
    "    max_margin = torch.maximum(margin-d_sqrt, torch.tensor(0))\n",
    "    \n",
    "    max_part = torch.square(max_margin)\n",
    "    second_part = torch.matmul(label, max_part)  # (Y) * max(margin - d, 0)\n",
    "\n",
    "    loss = 0.5 * torch.mean(first_part + second_part)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def compute_euclidean_distance(x, y):\n",
    "    \"\"\"\n",
    "    Computes the euclidean distance between two tensorflow variables\n",
    "    \"\"\"\n",
    "\n",
    "    d = torch.sum(torch.square(torch.sub(x, y)),1)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(963.6011, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_contrastive_loss(output1, output2, labels, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_contrastive_loss(left_feature, right_feature, label, margin):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the contrastive loss as in\n",
    "\n",
    "\n",
    "    L = 0.5 * Y * D^2 + 0.5 * (Y-1) * {max(0, margin - D)}^2\n",
    "\n",
    "    **Parameters**\n",
    "     left_feature: First element of the pair\n",
    "     right_feature: Second element of the pair\n",
    "     label: Label of the pair (0 or 1)\n",
    "     margin: Contrastive margin\n",
    "\n",
    "    **Returns**\n",
    "     Return the loss operation\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # -y * log(sim) + (1-y)*log(1-sim)\n",
    "\n",
    "    label = label.float()\n",
    "    \n",
    "    cosine = torch.nn.CosineSimilarity()\n",
    "    \n",
    "    sim = torch.mean(torch.square(cosine(left_feature, right_feature))).resize(1)\n",
    "    one = 1.0\n",
    "   \n",
    "    loss = torch.matmul(-label, torch.log(sim)) + torch.matmul((one-label).double(), torch.log(one-sim).double())\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "l = compute_contrastive_loss(output1[1], output2[1], labels[1].resize(1), 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.9170, dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l + l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "both arguments to matmul need to be at least 1D, but they are 1D and 0D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[246], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m l \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_contrastive_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[244], line 30\u001b[0m, in \u001b[0;36mcompute_contrastive_loss\u001b[0;34m(left_feature, right_feature, label, margin)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(sim\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(label\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m---> 30\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43msim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul((one\u001b[38;5;241m-\u001b[39mlabels)\u001b[38;5;241m.\u001b[39mdouble(), torch\u001b[38;5;241m.\u001b[39mlog(one\u001b[38;5;241m-\u001b[39msim)\u001b[38;5;241m.\u001b[39mdouble())\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[0;31mRuntimeError\u001b[0m: both arguments to matmul need to be at least 1D, but they are 1D and 0D"
     ]
    }
   ],
   "source": [
    "l = compute_contrastive_loss(output1, output2, labels, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[1].resize(1).float().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ContrastiveLoss(output1, output2, labels):\n",
    "    \n",
    "    loss = torch.tensor(0.0)\n",
    "    \n",
    "    for i in range(output1.size(0)):\n",
    "    \n",
    "        loss += compute_contrastive_loss(output1[i], output2[i], labels[i].resize(1), 0.1)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-16.8239, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ContrastiveLoss(output1, output2, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ede013c901f8a5774d468d6246940287a78a31eaa126b6cb7e1aa2724460e30f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
