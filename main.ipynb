{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_metric_learning import losses\n",
    "import data_handler\n",
    "from siamese_network import SiameseNetwork, train\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "from custom_losses import ContrastiveLoss\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data, _ = data_handler.load(path=\"dataset/\", filename_train=\"train.csv\", sep_char='#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['label'].value_counts().plot.bar(title='Labels Proportions in Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a stratified split dividing it into 80% training set and 20% validation set\n",
    "df_train, df_val = data_handler.split_train_data(df_data, perc_split=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['label'].value_counts().plot.bar(title='Labels Proportions in Training set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val['label'].value_counts().plot.bar(title='Labels Proportions in Validation set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate topics and keypoints, as stated in the paper\n",
    "df_train = data_handler.concatenate_topics(df_train)\n",
    "df_val = data_handler.concatenate_topics(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()\n",
    "print('--------')\n",
    "df_val.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[12850:13350]\n",
    "df_train = df_train.reset_index()\n",
    "#df_train[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = pd.DataFrame()\n",
    "n_words['words_per_arg_train'] = df_train['args'].str.split().apply(len)\n",
    "n_words['words_per_arg_val'] = df_val['args'].str.split().apply(len)\n",
    "n_words['words_per_kp_train'] = df_train['key_points'].str.split().apply(len)\n",
    "n_words['words_per_kp_val'] = df_val['key_points'].str.split().apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerazioni varie..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words.plot.box(figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our model's (bert-base-uncased) tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Tokenize data\n",
    "tokenized = data_handler.tokenize_df(df_train, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Vocabulary size of tokenizer:', tokenizer.vocab_size, '\\nContext size:', tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseNetwork(bert_type=BertModel.from_pretrained('bert-base-uncased'))\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(tokenized, shuffle=True, batch_size=batch_size, pin_memory=True)\n",
    "\n",
    "#train_loss = ContrastiveLoss()\n",
    "train_loss = losses.ContrastiveLoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "# Batch size: 16, 32\n",
    "# Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
    "# Number of epochs: 2, 3, 4\n",
    "\n",
    "# The BERT authors recommend between 2 and 4.\n",
    "epochs = 1\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "-y \u0001 log(^y) + (1 - y) \u0001 log(1 - ^y)\n",
    "where ^y is the cosine similarity of the embeddings,\n",
    "and y reflects whether a pair matches (1) or not (0).\n",
    "'''\n",
    "def paper_contrastive(cosine, label):\n",
    "    \n",
    "    #cosine[cosine == 0] = 1e-8\n",
    "    #cosine[cosine == 1] = 0.99\n",
    "    \n",
    "    log_1 = torch.nan_to_num(torch.log(cosine), nan=-1e5)\n",
    "    log_2 = torch.nan_to_num(torch.log(1-cosine), nan=-1e5)\n",
    "    \n",
    "    #print(f'cosine {log_1.shape} {log_2.shape}')\n",
    "    \n",
    "    contr = torch.mul((-label).double(), log_1.double())+ \\\n",
    "            torch.mul((1-label).double(), log_2.double())\n",
    "    return contr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "5+2\n",
    "#epoch.to(device)\n",
    "#train_loader.to(device)\n",
    "#optimizer.to(device)\n",
    "#scheduler.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_obj = torch.nn.MSELoss(reduction=\"none\")\n",
    "loss_obj = torch.nn.MSELoss()\n",
    "\n",
    "loss_obj.to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "epoch=1\n",
    "for batch_idx, (encodings) in enumerate(train_loader):\n",
    "  #images_1, images_2, targets = images_1.to(device), images_2.to(device), targets.to(device)\n",
    "\n",
    "  # Extract arguments, key_points and labels all from the same batch\n",
    "    #args = encodings['arg']\n",
    "    args = {k:v.to(device) for k,v in encodings['arg'].items()}\n",
    "    \n",
    "    #kps = encodings['kp']\n",
    "    kps = {k:v.to(device) for k,v in encodings['kp'].items()}\n",
    "    \n",
    "    labels = encodings['label']\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output1, output2 = model(args, kps)\n",
    "        \n",
    "    # AVG of every token\n",
    "    output1 = torch.mean(output1, 1)\n",
    "    output2 = torch.mean(output2, 1)\n",
    "    \n",
    "    cos = torch.nn.CosineSimilarity()\n",
    "    cosine_sim = cos(output1, output2)\n",
    "    \n",
    "    print(args['input_ids'].shape[0])\n",
    "    loss = loss_obj(cosine_sim.float(), labels.float())\n",
    "    #loss = torch.Tensor(args['input_ids'].shape[0], 1)\n",
    "    #for i in range(0, cosine_sim.shape[0]):\n",
    "    #    loss[i] = paper_contrastive(cosine_sim[i], labels[i])\n",
    "    #loss_function(output1, output2, labels)\n",
    "    print(loss.shape)\n",
    "\n",
    "    #loss = loss_function(tf.convert_to_tensor(labels.numpy()), tf.convert_to_tensor(outputs.numpy()))\n",
    "    #loss.mean().backward()\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip the norm of the gradients to 1.0.\n",
    "    # This is to help prevent the \"exploding gradients\" problem.\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # Update parameters and take a step using the computed gradient.\n",
    "    # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "    # modified based on their gradients, the learning rate, etc.\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update the learning rate.\n",
    "    scheduler.step()\n",
    "\n",
    "    if batch_idx>-1:\n",
    "        print(f'Train Epoch:', epoch, 'batch:',\n",
    "            batch_idx, 'loss:',\n",
    "            loss.mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = DataLoader(tokenized, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch_idx, (encodings) in enumerate(test):\n",
    "      #images_1, images_2, targets = images_1.to(device), images_2.to(device), targets.to(device)\n",
    "\n",
    "      # Extract arguments, key_points and labels all from the same batch\n",
    "        args = {k:v.to(device) for k,v in encodings['arg'].items()}\n",
    "\n",
    "        #kps = encodings['kp']\n",
    "        kps = {k:v.to(device) for k,v in encodings['kp'].items()}\n",
    "\n",
    "        labels = encodings['label']\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output1, output2 = model(args, kps)\n",
    "\n",
    "        # AVG of every token\n",
    "        output1 = torch.mean(output1, 1)\n",
    "        output2 = torch.mean(output2, 1)\n",
    "\n",
    "        cos = torch.nn.CosineSimilarity()\n",
    "        cosine_sim = cos(output1, output2)\n",
    "        print(f'cosine {cosine_sim} == {labels}')\n",
    "\n",
    "        loss = torch.Tensor(args['input_ids'].shape[0], 1)\n",
    "        for i in range(0, cosine_sim.shape[0]):\n",
    "            loss[i] = loss_obj(cosine_sim[i], labels[i])\n",
    "        #loss_function(output1, output2, labels)\n",
    "        print(f'loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, None, train_loader, ContrastiveLoss, optimizer, epoch, scheduler)\n",
    "    #test(model, device, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ContrastiveLoss(output1, output2, labels):\n",
    "    \n",
    "    loss = torch.tensor(0.0)\n",
    "    \n",
    "    for i in range(output1.size(0)):\n",
    "    \n",
    "        loss += compute_contrastive_loss(output1[i], output2[i], labels[i].resize(1), 0.1)\n",
    "        print(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ede013c901f8a5774d468d6246940287a78a31eaa126b6cb7e1aa2724460e30f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
