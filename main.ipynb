{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_metric_learning import losses\n",
    "import data_handler\n",
    "from siamese_network import SiameseNetwork, train\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "from custom_losses import ContrastiveLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch cuda available: True\n"
     ]
    }
   ],
   "source": [
    "torch.zeros(1).cuda()\n",
    "#print(f\"torch version: {torch.__version__}\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "print(f\"torch cuda available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data, _ = data_handler.load(path=\"dataset/\", filename_train=\"train.csv\", sep_char='#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero_train:  13100\n",
      "one_train:  3408\n",
      "zero_val:  3275\n",
      "one_val:  852\n"
     ]
    }
   ],
   "source": [
    "df_train, df_val = data_handler.split_train_data(df_data, perc_split=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = data_handler.concatenate_topics(df_train)\n",
    "df_val = data_handler.concatenate_topics(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16508 entries, 0 to 16507\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   args        16508 non-null  object \n",
      " 1   key_points  16508 non-null  object \n",
      " 2   labels      16508 non-null  float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 387.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[13200:13500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>args</th>\n",
       "      <th>key_points</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13200</td>\n",
       "      <td>prostitution happens; legalizing it would make...</td>\n",
       "      <td>We should legalize prostitution Legalizing sex...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13201</td>\n",
       "      <td>somebody was working hard to make something an...</td>\n",
       "      <td>We should abolish intellectual property rights...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13202</td>\n",
       "      <td>urbanization destroys natural habits, impactin...</td>\n",
       "      <td>We should fight urbanization Urbanization harm...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13203</td>\n",
       "      <td>we should not adopt atheism as people have the...</td>\n",
       "      <td>We should adopt atheism Atheism discriminates ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13204</td>\n",
       "      <td>cannabis is a wonder drug for people who suffe...</td>\n",
       "      <td>We should legalize cannabis Cannabis is safe/h...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>13295</td>\n",
       "      <td>journalism must reach all citizens and the cen...</td>\n",
       "      <td>We should subsidize journalism Journalism is i...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>13296</td>\n",
       "      <td>Journalism is the heart of a nation developmen...</td>\n",
       "      <td>We should subsidize journalism Journalism is i...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>13297</td>\n",
       "      <td>compulsory voting does not promote democratic ...</td>\n",
       "      <td>We should introduce compulsory voting Compulso...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>13298</td>\n",
       "      <td>parents should be able to utilize any technolo...</td>\n",
       "      <td>We should legalize sex selection It is within ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>13299</td>\n",
       "      <td>There is plenty of private funding available f...</td>\n",
       "      <td>We should subsidize space exploration Space ex...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                                               args  \\\n",
       "0   13200  prostitution happens; legalizing it would make...   \n",
       "1   13201  somebody was working hard to make something an...   \n",
       "2   13202  urbanization destroys natural habits, impactin...   \n",
       "3   13203  we should not adopt atheism as people have the...   \n",
       "4   13204  cannabis is a wonder drug for people who suffe...   \n",
       "..    ...                                                ...   \n",
       "95  13295  journalism must reach all citizens and the cen...   \n",
       "96  13296  Journalism is the heart of a nation developmen...   \n",
       "97  13297  compulsory voting does not promote democratic ...   \n",
       "98  13298  parents should be able to utilize any technolo...   \n",
       "99  13299  There is plenty of private funding available f...   \n",
       "\n",
       "                                           key_points  labels  \n",
       "0   We should legalize prostitution Legalizing sex...     1.0  \n",
       "1   We should abolish intellectual property rights...     1.0  \n",
       "2   We should fight urbanization Urbanization harm...     1.0  \n",
       "3   We should adopt atheism Atheism discriminates ...     1.0  \n",
       "4   We should legalize cannabis Cannabis is safe/h...     1.0  \n",
       "..                                                ...     ...  \n",
       "95  We should subsidize journalism Journalism is i...     1.0  \n",
       "96  We should subsidize journalism Journalism is i...     1.0  \n",
       "97  We should introduce compulsory voting Compulso...     1.0  \n",
       "98  We should legalize sex selection It is within ...     1.0  \n",
       "99  We should subsidize space exploration Space ex...     1.0  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "tokenized = data_handler.tokenize_df(df_train[:100], BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSiameseNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbert_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBertModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(tokenized, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#train_loss = ContrastiveLoss()\u001b[39;00m\n",
      "File \u001b[0;32m~/HLTKeyPointAnalysis/siamese_network.py:14\u001b[0m, in \u001b[0;36mSiameseNetwork.__init__\u001b[0;34m(self, bert_type, output_type)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, bert_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, output_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSiameseNetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bert_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m BertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m                                       num_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "model = SiameseNetwork(bert_type=BertModel.from_pretrained('bert-base-uncased'))\n",
    "\n",
    "train_loader = DataLoader(tokenized, shuffle=False, batch_size=32)\n",
    "\n",
    "#train_loss = ContrastiveLoss()\n",
    "train_loss = losses.ContrastiveLoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "# Batch size: 16, 32\n",
    "# Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
    "# Number of epochs: 2, 3, 4\n",
    "\n",
    "# The BERT authors recommend between 2 and 4.\n",
    "epochs = 1\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Train Epoch: 1 batch: 0 / 100 loss: -36.10004806518555\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, None, train_loader, ContrastiveLoss, optimizer, epoch, scheduler)\n",
    "    #test(model, device, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, encodings in enumerate(train_loader):\n",
    "    args = encodings['arg']\n",
    "    kps = encodings['kp']\n",
    "    labels = encodings['label']\n",
    "    if i==0:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1, output2 = model(args, kps, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-35.8793, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ContrastiveLoss(output1, output2, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 768])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[ [1],[3]], [[3],[5]]]).float()\n",
    "e = torch.tensor([[[1], [2]], [[3],[3]]]).float()\n",
    "l = torch.tensor([[1],[0]]).float()\n",
    "a[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan, dtype=torch.float64)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_contrastive_loss(a[0],e[0],l[0], 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_contrastive_loss(left_feature, right_feature, label, margin):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the contrastive loss as in\n",
    "\n",
    "\n",
    "    L = 0.5 * Y * D^2 + 0.5 * (Y-1) * {max(0, margin - D)}^2\n",
    "\n",
    "    **Parameters**\n",
    "     left_feature: First element of the pair\n",
    "     right_feature: Second element of the pair\n",
    "     label: Label of the pair (0 or 1)\n",
    "     margin: Contrastive margin\n",
    "\n",
    "    **Returns**\n",
    "     Return the loss operation\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    label = label.float()\n",
    "    one = 1.0\n",
    "\n",
    "    d = compute_euclidean_distance(left_feature, right_feature)\n",
    "    d_sqrt = torch.sqrt(compute_euclidean_distance(left_feature, right_feature))\n",
    "    first_part = torch.matmul(one-label, d)# (Y-1)*(d)\n",
    "\n",
    "    max_margin = torch.maximum(margin-d_sqrt, torch.tensor(0))\n",
    "    \n",
    "    max_part = torch.square(max_margin)\n",
    "    second_part = torch.matmul(label, max_part)  # (Y) * max(margin - d, 0)\n",
    "\n",
    "    loss = 0.5 * torch.mean(first_part + second_part)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def compute_euclidean_distance(x, y):\n",
    "    \"\"\"\n",
    "    Computes the euclidean distance between two tensorflow variables\n",
    "    \"\"\"\n",
    "\n",
    "    d = torch.sum(torch.square(torch.sub(x, y)),1)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_contrastive_loss(left_feature, right_feature, label, margin):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the contrastive loss as in\n",
    "\n",
    "\n",
    "    L = 0.5 * Y * D^2 + 0.5 * (Y-1) * {max(0, margin - D)}^2\n",
    "\n",
    "    **Parameters**\n",
    "     left_feature: First element of the pair\n",
    "     right_feature: Second element of the pair\n",
    "     label: Label of the pair (0 or 1)\n",
    "     margin: Contrastive margin\n",
    "\n",
    "    **Returns**\n",
    "     Return the loss operation\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # -y * log(sim) + (1-y)*log(1-sim)\n",
    "\n",
    "    label = label.float()\n",
    "    \n",
    "    cosine = torch.nn.CosineSimilarity()\n",
    "    \n",
    "    sim = torch.mean(torch.square(cosine(left_feature, right_feature))).resize(1)\n",
    "    one = 1.0\n",
    "   \n",
    "    loss = torch.matmul(-label, torch.log(sim)) + torch.matmul((one-label).double(), torch.log(one-sim).double())\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ContrastiveLoss(output1, output2, labels):\n",
    "    \n",
    "    loss = torch.tensor(0.0)\n",
    "    \n",
    "    for i in range(output1.size(0)):\n",
    "    \n",
    "        loss += compute_contrastive_loss(output1[i], output2[i], labels[i].resize(1), 0.1)\n",
    "        print(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-16.8239, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ContrastiveLoss(output1, output2, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([13100, 13101, 13102, 13103, 13104, 13105, 13106, 13107, 13108,\n",
       "            13109,\n",
       "            ...\n",
       "            16498, 16499, 16500, 16501, 16502, 16503, 16504, 16505, 16506,\n",
       "            16507],\n",
       "           dtype='int64', length=3408)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['labels'] == 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ede013c901f8a5774d468d6246940287a78a31eaa126b6cb7e1aa2724460e30f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
