{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb5d46e-0ff0-4fd3-9475-36f3695a4125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModel, DataCollatorForSeq2Seq\n",
    "from generative_model import GenerativeModel, train, test, validate\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from task2_utils import tokenize_df_gen, decode_data, compute_metrics, concat_tag\n",
    "import random\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(1, '../')\n",
    "import data_handler\n",
    "sys.path.insert(1, '../kp_match')\n",
    "import siamese_network \n",
    "from siamese_network import SiameseNetwork\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0833074-0a41-4a13-971b-3c7e336fff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f436461f-c497-483c-bcf4-4fd454287839",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "device = torch.device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87a027e1-7afb-4ee3-8713-2cabf3b06d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = data_handler.load_full_dataset('../dataset/', get_train=True, get_dev=True, get_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7004097-3bfa-4a38-a00e-3e12f5689ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate topics and keypoints, as stated in the paper\n",
    "df_train = data_handler.concatenate_topics(df_train, input_col='argument', output_col='argument')\n",
    "df_val = data_handler.concatenate_topics(df_val, input_col='argument', output_col='argument')\n",
    "df_test = data_handler.concatenate_topics(df_test, input_col='argument', output_col='argument')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0958713-00c3-4a91-ad1e-72124f3191ef",
   "metadata": {},
   "source": [
    "# Compute baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be2cb13-5376-46ed-8f1b-9e4dd04f8636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_baseline(model_type, device, loss, metrics):\n",
    "    \n",
    "    if model_type == 'google/pegasus-large':\n",
    "        tokenizer = AutoTokenizer.from_pretrained('google/pegasus-xsum')\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "    max_length = 100\n",
    "    \n",
    "    _, df_val, df_test = data_handler.load_full_dataset('../dataset/', get_train=False, get_dev=True, get_test=True)\n",
    "\n",
    "    df_test = df_test.sample(frac = 0.00027, random_state = 270898)\n",
    "    # Concatenate topics and keypoints, as stated in the paper\n",
    "    df_val = data_handler.concatenate_topics(df_val, input_col='argument', output_col='argument')\n",
    "    df_test = data_handler.concatenate_topics(df_test, input_col='argument', output_col='argument')\n",
    "    \n",
    "    if model_type == \"t5-small\" or model_type == \"t5-base\" or model_type == \"t5-large\":\n",
    "        df_val = concat_tag(df_val, 'argument')\n",
    "        df_test = concat_tag(df_test, 'argument')\n",
    "    \n",
    "    model = GenerativeModel(model_type)\n",
    "    model.to(device)\n",
    "    \n",
    "    tokenized_val = tokenize_df_gen(df_val, tokenizer, max_length=max_length)\n",
    "    tokenized_test = tokenize_df_gen(df_test, tokenizer, max_length=max_length, key_points_on=False)\n",
    "\n",
    "    seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True, max_length=max_length)\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        tokenized_val, # dataset di validazione\n",
    "        collate_fn=seq2seq_data_collator, # data collator\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_res = validate(model, device, val_loader, max_length=max_length)\n",
    "    \n",
    "    dec_pred, dec_exp = decode_data(val_res['predicted'], val_res['labels'], tokenizer)\n",
    "    # Compute metrics\n",
    "    val_metrics = compute_metrics(dec_pred, dec_exp, metrics)\n",
    "    print(f\"Validation results with model {model_type}:\")\n",
    "    print(val_metrics)\n",
    "    print(f\"Some validation phrases generated using {model_type}:\")\n",
    "    df_sample = df_val.sample(frac = 0.009, random_state = 270898)\n",
    "    index = df_sample.index\n",
    "    for i in index:\n",
    "        print(f\"Argument: {df_val['argument'].iloc[i]} \\nGenerated key-point: {dec_pred[i]}\\n\\n\")\n",
    "\n",
    "    print(\"----------------- TEST -----------------\")\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        tokenized_test, # dataset di validazione\n",
    "        collate_fn=seq2seq_data_collator, # data collator\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    test_res = test(model, device, test_loader, max_length=max_length)\n",
    "\n",
    "    dec_pred = tokenizer.batch_decode(test_res['predicted'].type(torch.IntTensor).cpu().data.numpy(), skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    print(f\"Some test phrases generated using {model_type}:\")\n",
    "    end = len(dec_pred)\n",
    "    for i in range(end):\n",
    "        print(f\"Argument: {df_test['argument'].iloc[i]} \\nGenerated key-point: {dec_pred[i]}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fbd8f9-05f9-4a30-b41a-ecf65a85fdae",
   "metadata": {},
   "source": [
    "# Pegasus Xsum baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63d2ef34-e6f2-4fa2-af8b-19f03a6e14d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "You're using a PegasusTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results with model google/pegasus-xsum:\n",
      "{'rouge': {'rouge1': {'precision': 0.06213652920676828, 'recall': 0.10197494484179298, 'fmeasure': 0.07226329623893364}, 'rouge2': {'precision': 0.005004212838704235, 'recall': 0.009373921670117321, 'fmeasure': 0.0061882023262247714}, 'rougeL': {'precision': 0.057589484741861985, 'recall': 0.09315528473137195, 'fmeasure': 0.06649627733578528}, 'rougeLsum': {'precision': 0.05764041082741468, 'recall': 0.09305865983855138, 'fmeasure': 0.06653920905986996}}}\n",
      "Some validation phrases generated using google/pegasus-xsum:\n",
      "Argument: The USA is a good country to live in It is an excellent country to live in due to its economy and possibilities of having a house, a car, a good job \n",
      "Generated key-point: What is your favourite country to live in?\n",
      "\n",
      "\n",
      "Argument: Routine child vaccinations should be mandatory They are against the freedom of parents to choose how to care for their children \n",
      "Generated key-point: Thousands of people have taken to the streets of London to protest against the government's decision to relax the rules on vaccinating children.\n",
      "\n",
      "\n",
      "Argument: The USA is a good country to live in Excellent country and excellent people. I would not change it for anything \n",
      "Generated key-point: What would you change about the United States of America?\n",
      "\n",
      "\n",
      "Argument: The USA is a good country to live in For something it is called the land of dreams where you must strive to achieve and live the American dream, land of freedom and free men where all men are equal. \n",
      "Generated key-point: The United States of America is the greatest country in the world.\n",
      "\n",
      "\n",
      "Argument: The USA is a good country to live in The USA is a good country to live in because you have the freedom to elect government office and to live your life the way you want. \n",
      "Generated key-point: The USA is a good country to live in because you have the freedom to elect government office and to live your life the way you want.\n",
      "\n",
      "\n",
      "----------------- TEST -----------------\n",
      "Some test phrases generated using google/pegasus-xsum:\n",
      "Argument: We should legalize polygamy polygamy, if legalized, could cause many issues with marital benefits, such as health insurance. \n",
      "Generated key-point: It is time for the UK to move away from a one-size-fits-all approach to family law.\n",
      "\n",
      "\n",
      "Argument: We should subsidize stay-at-home dads When dads stay at home, they are being supported by a working mom who gets paid less than men, so the father should be subsidized to make up the difference. \n",
      "Generated key-point: As a father of two young children, I would like to see more support for dads who stay at home to raise their children.\n",
      "\n",
      "\n",
      "Argument: We should ban algorithmic trading sometimes in a company the supply exceeds the demand so this process would work well. \n",
      "Generated key-point: algorithmic trading should be banned.\n",
      "\n",
      "\n",
      "Argument: We should subsidize embryonic stem cell research embryonic stem cell research is an unnatural act that goes against human decency.  the benefits do not justify the act. \n",
      "Generated key-point: The government should stop funding embryonic stem cell research.\n",
      "\n",
      "\n",
      "Argument: We should legalize polygamy polygamy should not be allowed as it demoralizes a woman in the since they they are not valued as the one and only woman that should be worshiped by a husband. \n",
      "Generated key-point: In our series of letters from African journalists, film-maker and columnist Farai Sevenzo considers polygamy.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_type = 'google/pegasus-xsum'\n",
    "test_baseline(model_type, device, None, ['rouge']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d2298a-90e0-49a1-bb29-3c941b198588",
   "metadata": {},
   "source": [
    "# T5 baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc2c10f8-853b-40e7-b6cb-f7675aef5de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results with model t5-large:\n",
      "{'rouge': {'rouge1': {'precision': 0.10961694906758337, 'recall': 0.295346156963005, 'fmeasure': 0.1545275636508756}, 'rouge2': {'precision': 0.034345113662356354, 'recall': 0.08198110766045547, 'fmeasure': 0.04687169373786172}, 'rougeL': {'precision': 0.10342556295835226, 'recall': 0.27779973649538947, 'fmeasure': 0.1456140366802304}, 'rougeLsum': {'precision': 0.10342266642318665, 'recall': 0.2771198004893661, 'fmeasure': 0.14555087425763585}}}\n",
      "Some validation phrases generated using t5-large:\n",
      "Argument: summarize: The USA is a good country to live in It is an excellent country to live in due to its economy and possibilities of having a house, a car, a good job \n",
      "Generated key-point: the usa is a good country to live in due to its economy and possibilities of having a house, a car, a good job. the\n",
      "\n",
      "\n",
      "Argument: summarize: Routine child vaccinations should be mandatory They are against the freedom of parents to choose how to care for their children \n",
      "Generated key-point: routine child vaccinations should be mandatory. they are against the freedom of parents to choose how to care for their children.\n",
      "\n",
      "\n",
      "Argument: summarize: The USA is a good country to live in Excellent country and excellent people. I would not change it for anything \n",
      "Generated key-point: the usa is a good country to live in excellent country and excellent people. the people are very friendly and helpful. the food is good and the people\n",
      "\n",
      "\n",
      "Argument: summarize: The USA is a good country to live in For something it is called the land of dreams where you must strive to achieve and live the American dream, land of freedom and free men where all men are equal. \n",
      "Generated key-point: the usa is a good country to live in. it is called the land of dreams. you must strive to achieve and live the american dream.\n",
      "\n",
      "\n",
      "Argument: summarize: The USA is a good country to live in The USA is a good country to live in because you have the freedom to elect government office and to live your life the way you want. \n",
      "Generated key-point: the usa is a good country to live in because you have the freedom to elect government office and to live your life the way you want.\n",
      "\n",
      "\n",
      "----------------- TEST -----------------\n",
      "Some test phrases generated using t5-large:\n",
      "Argument: summarize: We should legalize polygamy polygamy, if legalized, could cause many issues with marital benefits, such as health insurance. \n",
      "Generated key-point: legalizing polygamy could cause many issues with marital benefits, such as health insurance.\n",
      "\n",
      "\n",
      "Argument: summarize: We should subsidize stay-at-home dads When dads stay at home, they are being supported by a working mom who gets paid less than men, so the father should be subsidized to make up the difference. \n",
      "Generated key-point: stay-at-home dads are supported by a working mom who gets paid less than men. dads should be subsidized to make up the difference\n",
      "\n",
      "\n",
      "Argument: summarize: We should ban algorithmic trading sometimes in a company the supply exceeds the demand so this process would work well. \n",
      "Generated key-point: algorithmic trading is a way to make money in a company. algorithmic trading works well when supply exceeds demand.\n",
      "\n",
      "\n",
      "Argument: summarize: We should subsidize embryonic stem cell research embryonic stem cell research is an unnatural act that goes against human decency.  the benefits do not justify the act. \n",
      "Generated key-point: benefits do not justify the act. benefits are not a reason to subsidize research.\n",
      "\n",
      "\n",
      "Argument: summarize: We should legalize polygamy polygamy should not be allowed as it demoralizes a woman in the since they they are not valued as the one and only woman that should be worshiped by a husband. \n",
      "Generated key-point: polygamy should not be allowed as it demoralizes a woman. it makes her feel like she is not valued as the one and only woman \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_type = \"t5-large\"\n",
    "test_baseline(model_type, device, None, ['rouge']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3158f8b2-0d23-47bf-bc4c-628b52543258",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results with model t5-base:\n",
      "{'rouge': {'rouge1': {'precision': 0.08300883565918155, 'recall': 0.2624162170357828, 'fmeasure': 0.12233822003115435}, 'rouge2': {'precision': 0.024291782256521337, 'recall': 0.06780861801242233, 'fmeasure': 0.034930220379754635}, 'rougeL': {'precision': 0.07835370513523732, 'recall': 0.2476542730618822, 'fmeasure': 0.11523790671026327}, 'rougeLsum': {'precision': 0.07828706608247607, 'recall': 0.24756830076123587, 'fmeasure': 0.11510806771528925}}}\n",
      "Some validation phrases generated using t5-base:\n",
      "Argument: The USA is a good country to live in It is an excellent country to live in due to its economy and possibilities of having a house, a car, a good job \n",
      "Generated key-point: The USA is an excellent country to live in due to its economy and possibilities of having a house, a car, a good job, a good\n",
      "\n",
      "\n",
      "Argument: Routine child vaccinations should be mandatory They are against the freedom of parents to choose how to care for their children \n",
      "Generated key-point: vaccinations should be mandatory. Routine child vaccinations should be mandatory. Routine child vaccinations should be mandatory. Routine child vaccinations should be mandatory.\n",
      "\n",
      "\n",
      "Argument: The USA is a good country to live in Excellent country and excellent people. I would not change it for anything \n",
      "Generated key-point: is a good country to live in. is a good country to live in. The USA is a good country to live in. The people are\n",
      "\n",
      "\n",
      "Argument: The USA is a good country to live in For something it is called the land of dreams where you must strive to achieve and live the American dream, land of freedom and free men where all men are equal. \n",
      "Generated key-point: The USA is a good country to live in the land of dreams where you must strive to achieve and live the American dream, land of freedom and free men\n",
      "\n",
      "\n",
      "Argument: The USA is a good country to live in The USA is a good country to live in because you have the freedom to elect government office and to live your life the way you want. \n",
      "Generated key-point: The USA is a good country to live in is a good country to live in because you have the freedom to elect government office and to live your life\n",
      "\n",
      "\n",
      "----------------- TEST -----------------\n",
      "Some test phrases generated using t5-base:\n",
      "Argument: We should legalize polygamy polygamy, if legalized, could cause many issues with marital benefits, such as health insurance. \n",
      "Generated key-point: polygamy polygamy, if legalized, could cause many issues with marital benefits, such as health insurance.\n",
      "\n",
      "\n",
      "Argument: We should subsidize stay-at-home dads When dads stay at home, they are being supported by a working mom who gets paid less than men, so the father should be subsidized to make up the difference. \n",
      "Generated key-point: We should subsidize stay-at-home dads stay-at-home dads working moms who get paid less than men. subsidize\n",
      "\n",
      "\n",
      "Argument: We should ban algorithmic trading sometimes in a company the supply exceeds the demand so this process would work well. \n",
      "Generated key-point: We should ban algorithmic trading sometimes in a company the supply exceeds the demand so this process would work well.\n",
      "\n",
      "\n",
      "Argument: We should subsidize embryonic stem cell research embryonic stem cell research is an unnatural act that goes against human decency.  the benefits do not justify the act. \n",
      "Generated key-point: embryonic stem cell research embryonic stem cell research is an unnatural act that goes against human decency.\n",
      "\n",
      "\n",
      "Argument: We should legalize polygamy polygamy should not be allowed as it demoralizes a woman in the since they they are not valued as the one and only woman that should be worshiped by a husband. \n",
      "Generated key-point: We should legalize polygamy polygamy should not be allowed as it demoralizes a woman in the since they are not valued as the\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_type = 't5-base'\n",
    "test_baseline(model_type, device, None, ['rouge']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84e678c4-483e-4a1a-bede-518fd42192a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results with model t5-base:\n",
      "{'rouge': {'rouge1': {'precision': 0.1116564733688184, 'recall': 0.28190457891544907, 'fmeasure': 0.1535658125106884}, 'rouge2': {'precision': 0.03344998424324974, 'recall': 0.07069746376811595, 'fmeasure': 0.0433097133466568}, 'rougeL': {'precision': 0.10411177147672948, 'recall': 0.2621146767885907, 'fmeasure': 0.14294878804432842}, 'rougeLsum': {'precision': 0.10434581847968108, 'recall': 0.26270292207792273, 'fmeasure': 0.14319330067010871}}}\n",
      "Some validation phrases generated using t5-base:\n",
      "Argument: summarize: The USA is a good country to live in It is an excellent country to live in due to its economy and possibilities of having a house, a car, a good job \n",
      "Generated key-point: the USA is an excellent country to live in due to its economy and possibilities of having a house, a car, a good job. the economy is\n",
      "\n",
      "\n",
      "Argument: summarize: Routine child vaccinations should be mandatory They are against the freedom of parents to choose how to care for their children \n",
      "Generated key-point: adolfo valencia: routine child vaccinations should be mandatory. valencia: parents should be able to choose how\n",
      "\n",
      "\n",
      "Argument: summarize: The USA is a good country to live in Excellent country and excellent people. I would not change it for anything \n",
      "Generated key-point: the USA is a good country to live in Excellent country and excellent people.\n",
      "\n",
      "\n",
      "Argument: summarize: The USA is a good country to live in For something it is called the land of dreams where you must strive to achieve and live the American dream, land of freedom and free men where all men are equal. \n",
      "Generated key-point: the USA is a good country to live in. it is called the land of dreams where you must strive to achieve and live the American dream.\n",
      "\n",
      "\n",
      "Argument: summarize: The USA is a good country to live in The USA is a good country to live in because you have the freedom to elect government office and to live your life the way you want. \n",
      "Generated key-point: the USA is a good country to live in because you have the freedom to elect government office.\n",
      "\n",
      "\n",
      "----------------- TEST -----------------\n",
      "Some test phrases generated using t5-base:\n",
      "Argument: summarize: We should legalize polygamy polygamy, if legalized, could cause many issues with marital benefits, such as health insurance. \n",
      "Generated key-point: aaron carroll: polygamy, if legalized, could cause many issues with marital benefits. he says it could cause health insurance\n",
      "\n",
      "\n",
      "Argument: summarize: We should subsidize stay-at-home dads When dads stay at home, they are being supported by a working mom who gets paid less than men, so the father should be subsidized to make up the difference. \n",
      "Generated key-point: bob greene: stay-at-home dads are supported by a working mom who gets paid less than men. he says dads should be\n",
      "\n",
      "\n",
      "Argument: summarize: We should ban algorithmic trading sometimes in a company the supply exceeds the demand so this process would work well. \n",
      "Generated key-point: aaron carroll: we should ban algorithmic trading sometimes in a company the supply exceeds the demand. he says we should ban algorithmic\n",
      "\n",
      "\n",
      "Argument: summarize: We should subsidize embryonic stem cell research embryonic stem cell research is an unnatural act that goes against human decency.  the benefits do not justify the act. \n",
      "Generated key-point: aaron carroll: subsidize embryonic stem cell research is an unnatural act that goes against human decency. carroll: benefits do not\n",
      "\n",
      "\n",
      "Argument: summarize: We should legalize polygamy polygamy should not be allowed as it demoralizes a woman in the since they they are not valued as the one and only woman that should be worshiped by a husband. \n",
      "Generated key-point: polygamy should not be allowed as it demoralizes a woman. polygamy should not be allowed as it demoralizes a woman\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_type = 't5-base'\n",
    "test_baseline(model_type, device, None, ['rouge']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9405b9-30c0-4827-869d-52c8e11a6512",
   "metadata": {},
   "source": [
    "# Pegasus large baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2939f0de-d713-49e2-8d51-0740796ada21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PegasusTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results with model google/pegasus-large:\n",
      "{'rouge': {'rouge1': {'precision': 0.10820963088972507, 'recall': 0.3143958999937265, 'fmeasure': 0.15573026472146867}, 'rouge2': {'precision': 0.03366233124541992, 'recall': 0.08740654474350124, 'fmeasure': 0.04722381748122127}, 'rougeL': {'precision': 0.10105335298347559, 'recall': 0.2929210257335261, 'fmeasure': 0.1451897431166692}, 'rougeLsum': {'precision': 0.10055656289123927, 'recall': 0.2912791867954916, 'fmeasure': 0.1448048784387525}}}\n",
      "Some validation phrases generated using google/pegasus-large:\n",
      "Argument: The USA is a good country to live in It is an excellent country to live in due to its economy and possibilities of having a house, a car, a good job \n",
      "Generated key-point: The USA is a good country to live in It is an excellent country to live in due to its economy and possibilities of having a house, a car, a good\n",
      "\n",
      "\n",
      "Argument: Routine child vaccinations should be mandatory They are against the freedom of parents to choose how to care for their children \n",
      "Generated key-point: Routine child vaccinations should be mandatory They are against the freedom of parents to choose how to care for their children\n",
      "\n",
      "\n",
      "Argument: The USA is a good country to live in Excellent country and excellent people. I would not change it for anything \n",
      "Generated key-point: I would not change it for anything\n",
      "\n",
      "\n",
      "Argument: The USA is a good country to live in For something it is called the land of dreams where you must strive to achieve and live the American dream, land of freedom and free men where all men are equal. \n",
      "Generated key-point: The USA is a good country to live in For something it is called the land of dreams where you must strive to achieve and live the American dream, land of freedom\n",
      "\n",
      "\n",
      "Argument: The USA is a good country to live in The USA is a good country to live in because you have the freedom to elect government office and to live your life the way you want. \n",
      "Generated key-point: The USA is a good country to live in The USA is a good country to live in because you have the freedom to elect government office and to live your life the\n",
      "\n",
      "\n",
      "----------------- TEST -----------------\n",
      "Some test phrases generated using google/pegasus-large:\n",
      "Argument: We should legalize polygamy polygamy, if legalized, could cause many issues with marital benefits, such as health insurance. \n",
      "Generated key-point: We should legalize polygamy polygamy, if legalized, could cause many issues with marital benefits, such as health insurance.\n",
      "\n",
      "\n",
      "Argument: We should subsidize stay-at-home dads When dads stay at home, they are being supported by a working mom who gets paid less than men, so the father should be subsidized to make up the difference. \n",
      "Generated key-point: We should subsidize stay-at-home dads When dads stay at home, they are being supported by a working mom who gets paid less than men, so the father\n",
      "\n",
      "\n",
      "Argument: We should ban algorithmic trading sometimes in a company the supply exceeds the demand so this process would work well. \n",
      "Generated key-point: We should ban algorithmic trading sometimes in a company the supply exceeds the demand so this process would work well.\n",
      "\n",
      "\n",
      "Argument: We should subsidize embryonic stem cell research embryonic stem cell research is an unnatural act that goes against human decency.  the benefits do not justify the act. \n",
      "Generated key-point: the benefits do not justify the act.\n",
      "\n",
      "\n",
      "Argument: We should legalize polygamy polygamy should not be allowed as it demoralizes a woman in the since they they are not valued as the one and only woman that should be worshiped by a husband. \n",
      "Generated key-point: We should legalize polygamy polygamy should not be allowed as it demoralizes a woman in the since they they are not valued as the one and only woman that should be\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_type = 'google/pegasus-large'\n",
    "test_baseline(model_type, device, None, ['rouge']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c7b2beb-7d2d-41d0-bbf9-d3a3a319f56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "You're using a PegasusTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results with model google/pegasus-large:\n",
      "{'rouge': {'rouge1': {'precision': 0.1079776916773516, 'recall': 0.3136999197461161, 'fmeasure': 0.15546191657327046}, 'rouge2': {'precision': 0.03364944046949771, 'recall': 0.08711252012882449, 'fmeasure': 0.04706266727136035}, 'rougeL': {'precision': 0.10055017361456503, 'recall': 0.2916582687642473, 'fmeasure': 0.14462688937428447}, 'rougeLsum': {'precision': 0.1009240652119679, 'recall': 0.291994446274338, 'fmeasure': 0.14518459134455952}}}\n",
      "Some validation phrases generated using google/pegasus-large:\n",
      "Argument: The USA is a good country to live in It is an excellent country to live in due to its economy and possibilities of having a house, a car, a good job \n",
      "Generated key-point: The USA is a good country to live in It is an excellent country to live in due to its economy and possibilities of having a house, a car, a good\n",
      "\n",
      "\n",
      "Argument: Routine child vaccinations should be mandatory They are against the freedom of parents to choose how to care for their children \n",
      "Generated key-point: Routine child vaccinations should be mandatory They are against the freedom of parents to choose how to care for their children\n",
      "\n",
      "\n",
      "Argument: The USA is a good country to live in Excellent country and excellent people. I would not change it for anything \n",
      "Generated key-point: I would not change it for anything\n",
      "\n",
      "\n",
      "Argument: The USA is a good country to live in For something it is called the land of dreams where you must strive to achieve and live the American dream, land of freedom and free men where all men are equal. \n",
      "Generated key-point: The USA is a good country to live in For something it is called the land of dreams where you must strive to achieve and live the American dream, land of freedom\n",
      "\n",
      "\n",
      "Argument: The USA is a good country to live in The USA is a good country to live in because you have the freedom to elect government office and to live your life the way you want. \n",
      "Generated key-point: The USA is a good country to live in The USA is a good country to live in because you have the freedom to elect government office and to live your life the\n",
      "\n",
      "\n",
      "----------------- TEST -----------------\n",
      "Some test phrases generated using google/pegasus-large:\n",
      "Argument: We should legalize polygamy polygamy, if legalized, could cause many issues with marital benefits, such as health insurance. \n",
      "Generated key-point: We should legalize polygamy polygamy, if legalized, could cause many issues with marital benefits, such as health insurance.\n",
      "\n",
      "\n",
      "Argument: We should subsidize stay-at-home dads When dads stay at home, they are being supported by a working mom who gets paid less than men, so the father should be subsidized to make up the difference. \n",
      "Generated key-point: We should subsidize stay-at-home dads When dads stay at home, they are being supported by a working mom who gets paid less than men, so the father\n",
      "\n",
      "\n",
      "Argument: We should ban algorithmic trading sometimes in a company the supply exceeds the demand so this process would work well. \n",
      "Generated key-point: We should ban algorithmic trading sometimes in a company the supply exceeds the demand so this process would work well.\n",
      "\n",
      "\n",
      "Argument: We should subsidize embryonic stem cell research embryonic stem cell research is an unnatural act that goes against human decency.  the benefits do not justify the act. \n",
      "Generated key-point: the benefits do not justify the act.\n",
      "\n",
      "\n",
      "Argument: We should legalize polygamy polygamy should not be allowed as it demoralizes a woman in the since they they are not valued as the one and only woman that should be worshiped by a husband. \n",
      "Generated key-point: We should legalize polygamy polygamy should not be allowed as it demoralizes a woman in the since they they are not valued as the one and only woman that should be\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_type = 'google/pegasus-large'\n",
    "test_baseline(model_type, device, None, ['rouge']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93415af1-02c9-495d-bd13-3163928d0bee",
   "metadata": {},
   "source": [
    "# Retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e34f4a2-44e4-4b7a-9458-f529ea4981f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def show_time():\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afd1b419-5fc5-48eb-8250-a8e04ac78b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_best_model(config, df_train, df_val, df_test, max_length, loss, metrics, device, save_val=\"\"):\n",
    "    \n",
    "    # Load the best model's tokenizer\n",
    "    if config['model_type'] == 'google/pegasus-large':\n",
    "        tokenizer = AutoTokenizer.from_pretrained('google/pegasus-xsum')\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(config['model_type'])\n",
    "    \n",
    "    #Tokenize data\n",
    "    tokenized_tr = tokenize_df_gen(df_train, tokenizer, max_length=max_length)\n",
    "    tokenized_val = tokenize_df_gen(df_val, tokenizer, max_length=max_length)\n",
    "    tokenized_test = tokenize_df_gen(df_test, tokenizer, max_length=max_length, key_points_on=False)\n",
    "    \n",
    "    train_loader = DataLoader(tokenized_tr, batch_size = config['batch_size'], shuffle = True, pin_memory=True)\n",
    "    val_loader = DataLoader(tokenized_val, pin_memory=True)\n",
    "    test_loader = DataLoader(tokenized_test, pin_memory=True)\n",
    "\n",
    "    model = GenerativeModel(config['model_type'])\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    total_steps = len(train_loader) * config['epochs']\n",
    "    \n",
    "    if config['optimizer'] == 'adamW':\n",
    "        optimizer= torch.optim.AdamW(model.parameters(),\n",
    "                  lr = config['lr'], \n",
    "                  eps = config['eps'],\n",
    "                  weight_decay = config['weight_decay'])\n",
    "\n",
    "    # Create the learning rate scheduler.\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                        num_warmup_steps = config['warmup_steps'],\n",
    "                                        num_training_steps = total_steps)\n",
    "    \n",
    "    \n",
    "    print(\"Starting Training!\")\n",
    "    # Train best model\n",
    "    show_time()\n",
    "    train_res = train(model, device, train_loader, optimizer, config['epochs'], loss, scheduler, max_length, verbose=False)\n",
    "    print(\"Training ended!\")\n",
    "    show_time()\n",
    "    \n",
    "    train_scores = [None] * len(train_res['predicted'])\n",
    "    for i, elem in enumerate(train_res['predicted']):\n",
    "        dec_pred, dec_exp = decode_data(elem, train_res['labels'][i], tokenizer)\n",
    "        # Compute metrics\n",
    "        train_scores[i] = compute_metrics(dec_pred, dec_exp, metrics)\n",
    "    \n",
    "    print(\"Train performances\")\n",
    "    print(train_scores)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Perform evaluation\n",
    "    val_res = validate(model, device, val_loader, max_length=max_length)\n",
    "    \n",
    "    # Compute metrics\n",
    "    dec_pred, dec_exp = decode_data(val_res['predicted'], val_res['labels'], tokenizer)\n",
    "    validation_scores = compute_metrics(dec_pred, dec_exp, metrics)\n",
    "    \n",
    "    print(\"Validation performances\")\n",
    "    print(validation_scores)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    if save_val != \"\":\n",
    "        df = pd.DataFrame()\n",
    "        df['predictions'] = dec_pred\n",
    "        df['expected'] = dec_exp\n",
    "        df.to_csv(save_val, sep='#')\n",
    "    \n",
    "    # Perform evaluation\n",
    "    test_res = test(model, device, test_loader, max_length=max_length)\n",
    "    \n",
    "    # Print test predictions\n",
    "    dec_pred = tokenizer.batch_decode(test_res['predicted'].type(torch.IntTensor).cpu().data.numpy(), skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    print(f\"# Some test phrases generated using {config['model_type']}:\")\n",
    "    end = len(dec_pred)\n",
    "    for i in range(end):\n",
    "        print(f\"Argument: {df_test['argument'].iloc[i]} \\nGenerated key-point: {dec_pred[i]}\\n\\n\")\n",
    "        \n",
    "    return train_scores, validation_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7dfc902-d06e-4224-8757-cd28eb12e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_var(res, set_name):\n",
    "    \n",
    "    arr = []\n",
    "    for el in res:\n",
    "            if len(el) >= 2:\n",
    "                el = el[-1]\n",
    "            if type(el) == dict:\n",
    "                arr.append(el['rouge']['rouge1']['fmeasure'])\n",
    "            else: \n",
    "                arr.append(el[0]['rouge']['rouge1']['fmeasure'])\n",
    "    \n",
    "    arr = np.array(arr)\n",
    "    vals = []\n",
    "    \n",
    "    for i in range(len(arr)):\n",
    "        vals.append(arr[i])\n",
    "\n",
    "    vals = np.array(vals)\n",
    "    print(f\"{set_name} Mean: {vals.mean()} and Variance: {vals.var()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d89fb19-65d2-44ab-b706-902fbd3bd8e8",
   "metadata": {},
   "source": [
    "### Pegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26ac2c11-b67b-4e20-8f6b-4e10a2d8fbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['model_type'] = 'google/pegasus-xsum'\n",
    "config['epochs'] = 1\n",
    "config['lr'] = 4e-4\n",
    "config['eps'] = 1e-6\n",
    "config['weight_decay'] = 1e-8\n",
    "config['warmup_steps'] = 1e2\n",
    "config['batch_size'] = 8\n",
    "config['optimizer'] = 'adamW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67a63002-b0c8-4bbe-ac28-9450763a53ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.sample(frac = 0.00018, random_state = 270898)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c94a94a-c4b4-4a60-b3dc-e34d205fa070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training!\n",
      "Current Time = 14:35:53\n",
      "Training ended!\n",
      "Current Time = 14:53:07\n",
      "Train performances\n",
      "[{'rouge': {'rouge1': {'precision': 0.30724058780341995, 'recall': 0.3620097093949123, 'fmeasure': 0.31785761313826877}, 'rouge2': {'precision': 0.2083250475335769, 'recall': 0.2514903167492144, 'fmeasure': 0.21942628878586368}, 'rougeL': {'precision': 0.3007181961678671, 'recall': 0.35445767148573204, 'fmeasure': 0.31116840764530984}, 'rougeLsum': {'precision': 0.3004019915214863, 'recall': 0.35429599037417026, 'fmeasure': 0.31093671758658303}}}]\n",
      "\n",
      "\n",
      "Validation performances\n",
      "{'rouge': {'rouge1': {'precision': 0.1591784435134822, 'recall': 0.18309427086601043, 'fmeasure': 0.1642876361800807}, 'rouge2': {'precision': 0.023830675664914786, 'recall': 0.025459368530020697, 'fmeasure': 0.023746484694542977}, 'rougeL': {'precision': 0.13371991134339328, 'recall': 0.14885487901792285, 'fmeasure': 0.1357002470673943}, 'rougeLsum': {'precision': 0.13366777713205188, 'recall': 0.14852742617688303, 'fmeasure': 0.13543312840974758}}}\n",
      "\n",
      "\n",
      "# Some test phrases generated using google/pegasus-xsum:\n",
      "Argument: We should legalize polygamy polygamy, if legalized, could cause many issues with marital benefits, such as health insurance. \n",
      "Generated key-point: A polygamous marriage is expensive\n",
      "\n",
      "\n",
      "Argument: We should subsidize stay-at-home dads When dads stay at home, they are being supported by a working mom who gets paid less than men, so the father should be subsidized to make up the difference. \n",
      "Generated key-point: A stay-at-home dads are better off financially than other workers\n",
      "\n",
      "\n",
      "Argument: We should ban algorithmic trading sometimes in a company the supply exceeds the demand so this process would work well. \n",
      "Generated key-point: algorithmic trading increases liquidity\n",
      "\n",
      "\n",
      "Starting Training!\n",
      "Current Time = 14:56:34\n",
      "Training ended!\n",
      "Current Time = 15:13:35\n",
      "Train performances\n",
      "[{'rouge': {'rouge1': {'precision': 0.2973195574290777, 'recall': 0.35283948522243747, 'fmeasure': 0.30938704482303714}, 'rouge2': {'precision': 0.19922198537480684, 'recall': 0.24195774280734755, 'fmeasure': 0.21093381274823186}, 'rougeL': {'precision': 0.29005290712055304, 'recall': 0.34447772643300767, 'fmeasure': 0.30198617522966187}, 'rougeLsum': {'precision': 0.28987647284597556, 'recall': 0.3441524685409732, 'fmeasure': 0.3019047814933153}}}]\n",
      "\n",
      "\n",
      "Validation performances\n",
      "{'rouge': {'rouge1': {'precision': 0.15881229603256503, 'recall': 0.18272554739946067, 'fmeasure': 0.1633322356056729}, 'rouge2': {'precision': 0.03150660103785103, 'recall': 0.03845252472969862, 'fmeasure': 0.03299489143710269}, 'rougeL': {'precision': 0.14006268460392562, 'recall': 0.16077274426459234, 'fmeasure': 0.14370104694235686}, 'rougeLsum': {'precision': 0.13966148659642952, 'recall': 0.16023014827362686, 'fmeasure': 0.14333336884210754}}}\n",
      "\n",
      "\n",
      "# Some test phrases generated using google/pegasus-xsum:\n",
      "Argument: We should legalize polygamy polygamy, if legalized, could cause many issues with marital benefits, such as health insurance. \n",
      "Generated key-point: Polygamy is not fair/discriminatory\n",
      "\n",
      "\n",
      "Argument: We should subsidize stay-at-home dads When dads stay at home, they are being supported by a working mom who gets paid less than men, so the father should be subsidized to make up the difference. \n",
      "Generated key-point: Stay-at-home dads are better off financially than those who choose to stay at home\n",
      "\n",
      "\n",
      "Argument: We should ban algorithmic trading sometimes in a company the supply exceeds the demand so this process would work well. \n",
      "Generated key-point: A ban on algorithmic trading increases liquidity\n",
      "\n",
      "\n",
      "Starting Training!\n",
      "Current Time = 15:17:06\n",
      "Training ended!\n",
      "Current Time = 15:34:14\n",
      "Train performances\n",
      "[{'rouge': {'rouge1': {'precision': 0.3140216682723942, 'recall': 0.3630071071885289, 'fmeasure': 0.3224466158338626}, 'rouge2': {'precision': 0.21002472132879033, 'recall': 0.25134273244664435, 'fmeasure': 0.2200890216021645}, 'rougeL': {'precision': 0.3069540797300022, 'recall': 0.3551965526192703, 'fmeasure': 0.31542366816184875}, 'rougeLsum': {'precision': 0.30647677892541547, 'recall': 0.35455381369997147, 'fmeasure': 0.31501810404753916}}}]\n",
      "\n",
      "\n",
      "Validation performances\n",
      "{'rouge': {'rouge1': {'precision': 0.15764480255169244, 'recall': 0.1954161176987266, 'fmeasure': 0.16912481519762745}, 'rouge2': {'precision': 0.03590384052340574, 'recall': 0.04125690131124912, 'fmeasure': 0.037134956856445245}, 'rougeL': {'precision': 0.13399172055861683, 'recall': 0.16489565687935281, 'fmeasure': 0.14326883780071892}, 'rougeLsum': {'precision': 0.1336151053082803, 'recall': 0.1642113748771361, 'fmeasure': 0.14287498299257606}}}\n",
      "\n",
      "\n",
      "# Some test phrases generated using google/pegasus-xsum:\n",
      "Argument: We should legalize polygamy polygamy, if legalized, could cause many issues with marital benefits, such as health insurance. \n",
      "Generated key-point: Polygamous relationships are unstable\n",
      "\n",
      "\n",
      "Argument: We should subsidize stay-at-home dads When dads stay at home, they are being supported by a working mom who gets paid less than men, so the father should be subsidized to make up the difference. \n",
      "Generated key-point: A stay-at-home dads are losing their childhood\n",
      "\n",
      "\n",
      "Argument: We should ban algorithmic trading sometimes in a company the supply exceeds the demand so this process would work well. \n",
      "Generated key-point: algorithmic trading increases liquidity\n",
      "\n",
      "\n",
      "Starting Training!\n",
      "Current Time = 15:38:08\n",
      "Training ended!\n",
      "Current Time = 15:55:43\n",
      "Train performances\n",
      "[{'rouge': {'rouge1': {'precision': 0.3148398310959848, 'recall': 0.37501928774062454, 'fmeasure': 0.32753399067453304}, 'rouge2': {'precision': 0.2142674713642125, 'recall': 0.26110789756409797, 'fmeasure': 0.22660305153188487}, 'rougeL': {'precision': 0.30761881265649554, 'recall': 0.3663600992344975, 'fmeasure': 0.32022263709540255}, 'rougeLsum': {'precision': 0.3076139229748137, 'recall': 0.36632610808781196, 'fmeasure': 0.32006830701840006}}}]\n",
      "\n",
      "\n",
      "Validation performances\n",
      "{'rouge': {'rouge1': {'precision': 0.14907746728756482, 'recall': 0.16150300233180703, 'fmeasure': 0.14826142385070756}, 'rouge2': {'precision': 0.020573521184934225, 'recall': 0.023989964343225208, 'fmeasure': 0.020908866992626576}, 'rougeL': {'precision': 0.13626772189057695, 'recall': 0.14689333160800594, 'fmeasure': 0.13521098648545204}, 'rougeLsum': {'precision': 0.1364842055774553, 'recall': 0.14694643118556197, 'fmeasure': 0.13546512243995196}}}\n",
      "\n",
      "\n",
      "# Some test phrases generated using google/pegasus-xsum:\n",
      "Argument: We should legalize polygamy polygamy, if legalized, could cause many issues with marital benefits, such as health insurance. \n",
      "Generated key-point: Polygamies are expensive\n",
      "\n",
      "\n",
      "Argument: We should subsidize stay-at-home dads When dads stay at home, they are being supported by a working mom who gets paid less than men, so the father should be subsidized to make up the difference. \n",
      "Generated key-point: A stay-at-home dads are better off financially than their counterparts\n",
      "\n",
      "\n",
      "Argument: We should ban algorithmic trading sometimes in a company the supply exceeds the demand so this process would work well. \n",
      "Generated key-point: A ban on algorithmic trading reduces the risk of losing money\n",
      "\n",
      "\n",
      "Starting Training!\n",
      "Current Time = 15:59:44\n",
      "Training ended!\n",
      "Current Time = 16:17:01\n",
      "Train performances\n",
      "[{'rouge': {'rouge1': {'precision': 0.2895513451788633, 'recall': 0.34716338083684883, 'fmeasure': 0.3025327840011779}, 'rouge2': {'precision': 0.1882447539977598, 'recall': 0.23333321930040005, 'fmeasure': 0.20059316146805917}, 'rougeL': {'precision': 0.28165821135200153, 'recall': 0.33800766254997905, 'fmeasure': 0.2944474869107111}, 'rougeLsum': {'precision': 0.2814118863187842, 'recall': 0.3377884635650232, 'fmeasure': 0.29427455849999046}}}]\n",
      "\n",
      "\n",
      "Validation performances\n",
      "{'rouge': {'rouge1': {'precision': 0.14882068858251762, 'recall': 0.17592697157914589, 'fmeasure': 0.15557871112374982}, 'rouge2': {'precision': 0.022649741060067133, 'recall': 0.02602728893489762, 'fmeasure': 0.023162586945355473}, 'rougeL': {'precision': 0.12611235294259632, 'recall': 0.14738338352468822, 'fmeasure': 0.13055497664602472}, 'rougeLsum': {'precision': 0.12568851133828796, 'recall': 0.14708243145743183, 'fmeasure': 0.13031988727075255}}}\n",
      "\n",
      "\n",
      "# Some test phrases generated using google/pegasus-xsum:\n",
      "Argument: We should legalize polygamy polygamy, if legalized, could cause many issues with marital benefits, such as health insurance. \n",
      "Generated key-point: A polygamy harms the family\n",
      "\n",
      "\n",
      "Argument: We should subsidize stay-at-home dads When dads stay at home, they are being supported by a working mom who gets paid less than men, so the father should be subsidized to make up the difference. \n",
      "Generated key-point: A stay-at-home dads are losing their childhood\n",
      "\n",
      "\n",
      "Argument: We should ban algorithmic trading sometimes in a company the supply exceeds the demand so this process would work well. \n",
      "Generated key-point: A ban on algorithmic trading is beneficial for the entire market/society\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr_metrics = []; val_metrics = [];\n",
    "save_res = \"\"\n",
    "for i in range(5):\n",
    "    if i == 4:\n",
    "        save_res = 'pegasus_val.csv'\n",
    "    tr, val = train_best_model(config, df_train, df_val, df_test, 100, None, ['rouge'], device, save_res)\n",
    "    tr_metrics.append(tr)\n",
    "    val_metrics.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7b6bb042-af6a-4ff9-8c5d-c3051aa2edd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Mean: 0.31595160969417585 and Variance: 8.062557959681058e-05\n",
      "Validation Mean: 0.1601169643915677 and Variance: 5.400468650616418e-05\n"
     ]
    }
   ],
   "source": [
    "set_name = \"Train\"; compute_mean_var(tr_metrics, set_name)\n",
    "set_name = \"Validation\"; compute_mean_var(val_metrics, set_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18541340-3892-4228-82fa-67290578db39",
   "metadata": {},
   "source": [
    "### T5 Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8eb20a3-c70c-4728-87b6-eefe6b47dbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['model_type'] = 't5-large'\n",
    "config['epochs'] = 1\n",
    "config['lr'] = 2e-5\n",
    "config['eps'] = 1e-8\n",
    "config['weight_decay'] = 0\n",
    "config['warmup_steps'] = 1e2\n",
    "config['batch_size'] = 4\n",
    "config['optimizer'] = 'adamW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ce08481-4d5b-4294-8140-c361624ba9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.sample(frac = 0.00018, random_state = 270898)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45765267-e5c5-42ef-809e-4e27c09eb87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_metrics = []; val_metrics = [];\n",
    "save_res = \"\"\n",
    "for i in range(5):\n",
    "    if i == 4:\n",
    "        save_res = 't5_large_val.csv'\n",
    "    tr, val = train_best_model(config, df_train, df_val, df_test, 100, None, ['rouge'], device, save_res)\n",
    "    tr_metrics.append(tr)\n",
    "    val_metrics.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9e50a3-8361-42ba-af1f-c2685ce6da2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_name = \"Train\"; compute_mean_var(tr_metrics, set_name)\n",
    "set_name = \"Validation\"; compute_mean_var(val_metrics, set_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a4c278-e9a1-4ef2-bc8e-048664933064",
   "metadata": {},
   "source": [
    "### T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "642ddecf-b48d-4808-9365-0ce2495a6ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['model_type'] = 't5-base'\n",
    "config['epochs'] = 2\n",
    "config['lr'] = 2e-4\n",
    "config['eps'] = 1e-8\n",
    "config['weight_decay'] = 1e-8\n",
    "config['warmup_steps'] = 1e3\n",
    "config['batch_size'] = 8\n",
    "config['optimizer'] = 'adamW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63931217-1e8b-4df5-86b0-2eac43bbdf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training!\n",
      "Current Time = 20:09:33\n",
      "Training ended!\n",
      "Current Time = 20:39:31\n",
      "Train performances\n",
      "[{'rouge': {'rouge1': {'precision': 0.12577384344795173, 'recall': 0.2746969256612811, 'fmeasure': 0.15838117619247208}, 'rouge2': {'precision': 0.047010391779388955, 'recall': 0.10255516961964206, 'fmeasure': 0.057968528150787735}, 'rougeL': {'precision': 0.1200437875287323, 'recall': 0.26224995420850183, 'fmeasure': 0.15105935853699304}, 'rougeLsum': {'precision': 0.12003250207809356, 'recall': 0.2622702363855435, 'fmeasure': 0.15109835342488542}}}, {'rouge': {'rouge1': {'precision': 0.25835363818930723, 'recall': 0.3350158589027651, 'fmeasure': 0.27517864270732534}, 'rouge2': {'precision': 0.15065301857208302, 'recall': 0.19756975663470605, 'fmeasure': 0.16034023911983222}, 'rougeL': {'precision': 0.2518020407206032, 'recall': 0.32624250943578453, 'fmeasure': 0.26811950927329675}, 'rougeLsum': {'precision': 0.2518631325765758, 'recall': 0.32625449272446144, 'fmeasure': 0.26828468002372086}}}]\n",
      "\n",
      "\n",
      "Validation performances\n",
      "{'rouge': {'rouge1': {'precision': 0.14232701834852796, 'recall': 0.17087816571512265, 'fmeasure': 0.1443406645784528}, 'rouge2': {'precision': 0.024562347723178092, 'recall': 0.029133238440303647, 'fmeasure': 0.025029479645910784}, 'rougeL': {'precision': 0.12806715523365275, 'recall': 0.15540844521822827, 'fmeasure': 0.13073156562646754}, 'rougeLsum': {'precision': 0.12804987614562785, 'recall': 0.15519307333166069, 'fmeasure': 0.1304259508168672}}}\n",
      "\n",
      "\n",
      "Some test phrases generated using t5-base:\n",
      "Argument: We should legalize polygamy polygamy, if legalized, could cause many issues with marital benefits, such as health insurance. \n",
      "Generated key-point: is not legalized yet\n",
      "\n",
      "\n",
      "Argument: We should subsidize stay-at-home dads When dads stay at home, they are being supported by a working mom who gets paid less than men, so the father should be subsidized to make up the difference. \n",
      "Generated key-point: stay-at-home dads are financially better off they can afford to stay at home with their children\n",
      "\n",
      "\n",
      "Argument: We should ban algorithmic trading sometimes in a company the supply exceeds the demand so this process would work well. \n",
      "Generated key-point: algorithmic trading is necessary to compensate for the loss of profits\n",
      "\n",
      "\n",
      "Argument: We should subsidize embryonic stem cell research embryonic stem cell research is an unnatural act that goes against human decency.  the benefits do not justify the act. \n",
      "Generated key-point: cell research is not ethical\n",
      "\n",
      "\n",
      "Argument: We should legalize polygamy polygamy should not be allowed as it demoralizes a woman in the since they they are not valued as the one and only woman that should be worshiped by a husband. \n",
      "Generated key-point: a polygamy is not fair/unhealthy for women\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_best_model(config, df_train, df_val, df_test, 100, None, ['rouge'], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4a8b2a7-6a20-4c76-bb81-d726e548e78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['model_type'] = 't5-base'\n",
    "config['epochs'] = 1\n",
    "config['lr'] = 5e-08\n",
    "config['eps'] = 1e-8\n",
    "config['weight_decay'] = 0\n",
    "config['warmup_steps'] = 1e2\n",
    "config['batch_size'] = 8\n",
    "config['optimizer'] = 'adamW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69e57e2e-1701-4367-9c66-ea7cafdce619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from task2_utils import concat_tag\n",
    "#Only for T5\n",
    "df_tr = df_train\n",
    "df_v = df_val\n",
    "df_te = df_test\n",
    "df_tr = concat_tag(df_tr, 'argument')\n",
    "df_v = concat_tag(df_v, 'argument')\n",
    "df_te = concat_tag(df_te, 'argument')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5486dcea-842c-4810-af62-5d53a69c96a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training!\n",
      "Current Time = 11:05:09\n",
      "Training ended!\n",
      "Current Time = 11:18:46\n",
      "Train performances\n",
      "[{'rouge': {'rouge1': {'precision': 0.09936448400961823, 'recall': 0.24309939570690886, 'fmeasure': 0.13320467739282305}, 'rouge2': {'precision': 0.026570084626291925, 'recall': 0.06723275945965022, 'fmeasure': 0.03545323520924892}, 'rougeL': {'precision': 0.09178418871449816, 'recall': 0.2260853974963144, 'fmeasure': 0.12307518503525874}, 'rougeLsum': {'precision': 0.09179090510333113, 'recall': 0.2261176320818551, 'fmeasure': 0.12315232293902334}}}]\n",
      "\n",
      "\n",
      "Validation performances\n",
      "{'rouge': {'rouge1': {'precision': 0.11195136857965979, 'recall': 0.2827202537800372, 'fmeasure': 0.15400119503718612}, 'rouge2': {'precision': 0.033458307206531174, 'recall': 0.07069746376811595, 'fmeasure': 0.04329504141425909}, 'rougeL': {'precision': 0.10425455477497961, 'recall': 0.2626301838258366, 'fmeasure': 0.14309073388623508}, 'rougeLsum': {'precision': 0.10448879503439072, 'recall': 0.2631421291068037, 'fmeasure': 0.14330523279846863}}}\n",
      "\n",
      "\n",
      "Some test phrases generated using t5-base:\n",
      "Argument: summarize: We should legalize polygamy polygamy, if legalized, could cause many issues with marital benefits, such as health insurance. \n",
      "Generated key-point: aaron carroll: polygamy, if legalized, could cause many issues with marital benefits. he says it could cause health insurance\n",
      "\n",
      "\n",
      "Argument: summarize: We should subsidize stay-at-home dads When dads stay at home, they are being supported by a working mom who gets paid less than men, so the father should be subsidized to make up the difference. \n",
      "Generated key-point: bob greene: stay-at-home dads are supported by a working mom who gets paid less than men. he says dads should be\n",
      "\n",
      "\n",
      "Argument: summarize: We should ban algorithmic trading sometimes in a company the supply exceeds the demand so this process would work well. \n",
      "Generated key-point: we should ban algorithmic trading sometimes in a company the supply exceeds the demand.\n",
      "\n",
      "\n",
      "Argument: summarize: We should subsidize embryonic stem cell research embryonic stem cell research is an unnatural act that goes against human decency.  the benefits do not justify the act. \n",
      "Generated key-point: aaron carroll: subsidize embryonic stem cell research is an unnatural act that goes against human decency. carroll: benefits do not\n",
      "\n",
      "\n",
      "Argument: summarize: We should legalize polygamy polygamy should not be allowed as it demoralizes a woman in the since they they are not valued as the one and only woman that should be worshiped by a husband. \n",
      "Generated key-point: polygamy should not be allowed as it demoralizes a woman. polygamy should not be allowed as it demoralizes a woman\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_best_model(config, df_tr, df_v, df_te, 100, None, ['rouge'], device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824c7f65-1ad1-4c1c-9c2c-93f41a15f5b0",
   "metadata": {},
   "source": [
    "### Pegasus Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e41619f-25fb-4439-a765-c130ef98ecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['model_type'] = 'google/pegasus-large'\n",
    "config['epochs'] = 1\n",
    "config['lr'] = 5e-4\n",
    "config['eps'] = 1e-8\n",
    "config['weight_decay'] = 1e-6\n",
    "config['warmup_steps'] = 1e3\n",
    "config['batch_size'] = 8\n",
    "config['optimizer'] = 'adamW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ffff8d2-4486-4879-98aa-72bb124e2893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training!\n",
      "Current Time = 20:59:19\n",
      "Training ended!\n",
      "Current Time = 21:16:59\n",
      "Train performances\n",
      "[{'rouge': {'rouge1': {'precision': 0.18183926160919697, 'recall': 0.23309780080863485, 'fmeasure': 0.18514979968414852}, 'rouge2': {'precision': 0.09583374205499902, 'recall': 0.12766603652619835, 'fmeasure': 0.10276293387582418}, 'rougeL': {'precision': 0.1755627003410492, 'recall': 0.2241461695955993, 'fmeasure': 0.1784404415138791}, 'rougeLsum': {'precision': 0.17570379543193, 'recall': 0.22407952524999306, 'fmeasure': 0.17848723751034817}}}]\n",
      "\n",
      "\n",
      "Validation performances\n",
      "{'rouge': {'rouge1': {'precision': 0.15293951054052973, 'recall': 0.18594011413304923, 'fmeasure': 0.15968759432984228}, 'rouge2': {'precision': 0.03507326887082321, 'recall': 0.03795505521048996, 'fmeasure': 0.03378925066987077}, 'rougeL': {'precision': 0.14019070757160124, 'recall': 0.16872666886253876, 'fmeasure': 0.14533371129964887}, 'rougeLsum': {'precision': 0.14031292677792073, 'recall': 0.16879261036869775, 'fmeasure': 0.1453957364875426}}}\n",
      "\n",
      "\n",
      "# Some test phrases generated using google/pegasus-large:\n",
      "Argument: We should legalize polygamy polygamy, if legalized, could cause many issues with marital benefits, such as health insurance. \n",
      "Generated key-point: should be legalized polygamy harms the state's image/reputation\n",
      "\n",
      "\n",
      "Argument: We should subsidize stay-at-home dads When dads stay at home, they are being supported by a working mom who gets paid less than men, so the father should be subsidized to make up the difference. \n",
      "Generated key-point: subsidizing a stay-at-at-home dads increases the quality of life for women\n",
      "\n",
      "\n",
      "Argument: We should ban algorithmic trading sometimes in a company the supply exceeds the demand so this process would work well. \n",
      "Generated key-point: Algorithm trading is necessary for the benefit of the entire market/society\n",
      "\n",
      "\n",
      "Starting Training!\n",
      "Current Time = 21:20:40\n",
      "Training ended!\n",
      "Current Time = 21:38:20\n",
      "Train performances\n",
      "[{'rouge': {'rouge1': {'precision': 0.18510413443310408, 'recall': 0.23640974052513627, 'fmeasure': 0.1874445937982357}, 'rouge2': {'precision': 0.09835691859330975, 'recall': 0.1310618187485112, 'fmeasure': 0.10395620227664207}, 'rougeL': {'precision': 0.1787781061961154, 'recall': 0.22671536604984932, 'fmeasure': 0.18063726881683256}, 'rougeLsum': {'precision': 0.17870122918378598, 'recall': 0.22692752175795358, 'fmeasure': 0.18069123502159357}}}]\n",
      "\n",
      "\n",
      "Validation performances\n",
      "{'rouge': {'rouge1': {'precision': 0.11692135794979532, 'recall': 0.1314809586548718, 'fmeasure': 0.11751538843289848}, 'rouge2': {'precision': 0.028051278934431087, 'recall': 0.02463444616977225, 'fmeasure': 0.02456534462251079}, 'rougeL': {'precision': 0.10078720217986539, 'recall': 0.1139062728736643, 'fmeasure': 0.10150132489404598}, 'rougeLsum': {'precision': 0.10089451355422002, 'recall': 0.11407952454148107, 'fmeasure': 0.1017240637909035}}}\n",
      "\n",
      "\n",
      "# Some test phrases generated using google/pegasus-large:\n",
      "Argument: We should legalize polygamy polygamy, if legalized, could cause many issues with marital benefits, such as health insurance. \n",
      "Generated key-point: We should not legalize polygamy\n",
      "\n",
      "\n",
      "Argument: We should subsidize stay-at-home dads When dads stay at home, they are being supported by a working mom who gets paid less than men, so the father should be subsidized to make up the difference. \n",
      "Generated key-point: STAY-at-home dads should be able to supplement their income if they choose to\n",
      "\n",
      "\n",
      "Argument: We should ban algorithmic trading sometimes in a company the supply exceeds the demand so this process would work well. \n",
      "Generated key-point: is necessary to reduce institutional trading\n",
      "\n",
      "\n",
      "Starting Training!\n",
      "Current Time = 21:42:12\n",
      "Training ended!\n",
      "Current Time = 22:00:34\n",
      "Train performances\n",
      "[{'rouge': {'rouge1': {'precision': 0.18309926158469936, 'recall': 0.24134464114205953, 'fmeasure': 0.18848620506896696}, 'rouge2': {'precision': 0.09918096821546932, 'recall': 0.1346338708676292, 'fmeasure': 0.10595420253611523}, 'rougeL': {'precision': 0.17697074033262156, 'recall': 0.2319551109654645, 'fmeasure': 0.18156240017773878}, 'rougeLsum': {'precision': 0.1770116755246171, 'recall': 0.2319086607614513, 'fmeasure': 0.18174934552323363}}}]\n",
      "\n",
      "\n",
      "Validation performances\n",
      "{'rouge': {'rouge1': {'precision': 0.16288113180096908, 'recall': 0.16389950880440038, 'fmeasure': 0.15706803737613184}, 'rouge2': {'precision': 0.04960640959282262, 'recall': 0.04964666724177588, 'fmeasure': 0.047548077455366405}, 'rougeL': {'precision': 0.15883759457129054, 'recall': 0.15900418522701154, 'fmeasure': 0.15283836285827268}, 'rougeLsum': {'precision': 0.15906541500970892, 'recall': 0.15959112344981935, 'fmeasure': 0.15309090157875177}}}\n",
      "\n",
      "\n",
      "# Some test phrases generated using google/pegasus-large:\n",
      "Argument: We should legalize polygamy polygamy, if legalized, could cause many issues with marital benefits, such as health insurance. \n",
      "Generated key-point: can lead to wrong sexual behaviour\n",
      "\n",
      "\n",
      "Argument: We should subsidize stay-at-home dads When dads stay at home, they are being supported by a working mom who gets paid less than men, so the father should be subsidized to make up the difference. \n",
      "Generated key-point: are necessary to compensate for the quality of life that a father should be able to provide\n",
      "\n",
      "\n",
      "Argument: We should ban algorithmic trading sometimes in a company the supply exceeds the demand so this process would work well. \n",
      "Generated key-point: can be used to reduce latency\n",
      "\n",
      "\n",
      "Starting Training!\n",
      "Current Time = 22:03:59\n",
      "Training ended!\n",
      "Current Time = 22:21:41\n",
      "Train performances\n",
      "[{'rouge': {'rouge1': {'precision': 0.17894832452842915, 'recall': 0.2415577683509853, 'fmeasure': 0.18759893458410729}, 'rouge2': {'precision': 0.0967876618044489, 'recall': 0.13768094908462825, 'fmeasure': 0.1060432073793704}, 'rougeL': {'precision': 0.17375936055177627, 'recall': 0.23312981272540095, 'fmeasure': 0.18177146559009683}, 'rougeLsum': {'precision': 0.1738263711906479, 'recall': 0.23335475521099747, 'fmeasure': 0.1818442425086761}}}]\n",
      "\n",
      "\n",
      "Validation performances\n",
      "{'rouge': {'rouge1': {'precision': 0.12397624804085666, 'recall': 0.15997886473429979, 'fmeasure': 0.13514621176187838}, 'rouge2': {'precision': 0.023178002050284648, 'recall': 0.027017195767195763, 'fmeasure': 0.02397153652622086}, 'rougeL': {'precision': 0.11367769160638375, 'recall': 0.14645552967563857, 'fmeasure': 0.12363850554563224}, 'rougeLsum': {'precision': 0.11387422615074438, 'recall': 0.1467026959554137, 'fmeasure': 0.1239037553682864}}}\n",
      "\n",
      "\n",
      "# Some test phrases generated using google/pegasus-large:\n",
      "Argument: We should legalize polygamy polygamy, if legalized, could cause many issues with marital benefits, such as health insurance. \n",
      "Generated key-point: benefits are important to a polygamous relationship\n",
      "\n",
      "\n",
      "Argument: We should subsidize stay-at-home dads When dads stay at home, they are being supported by a working mom who gets paid less than men, so the father should be subsidized to make up the difference. \n",
      "Generated key-point: encourage the child's education\n",
      "\n",
      "\n",
      "Argument: We should ban algorithmic trading sometimes in a company the supply exceeds the demand so this process would work well. \n",
      "Generated key-point: should not be banned as long as there is continuous improvement\n",
      "\n",
      "\n",
      "Starting Training!\n",
      "Current Time = 22:26:03\n",
      "Training ended!\n",
      "Current Time = 22:44:03\n",
      "Train performances\n",
      "[{'rouge': {'rouge1': {'precision': 0.16804465748257386, 'recall': 0.23836808005475424, 'fmeasure': 0.1761834751559399}, 'rouge2': {'precision': 0.08233024912867695, 'recall': 0.1264420892285273, 'fmeasure': 0.09127849522575135}, 'rougeL': {'precision': 0.16263128806083246, 'recall': 0.2286922246143436, 'fmeasure': 0.1698881418082651}, 'rougeLsum': {'precision': 0.1626527796427129, 'recall': 0.2288902127062387, 'fmeasure': 0.1698691131103542}}}]\n",
      "\n",
      "\n",
      "Validation performances\n",
      "{'rouge': {'rouge1': {'precision': 0.1324116600856198, 'recall': 0.14873188405797114, 'fmeasure': 0.1345840905162157}, 'rouge2': {'precision': 0.03298003702215657, 'recall': 0.030639665286404417, 'fmeasure': 0.030393216452869194}, 'rougeL': {'precision': 0.12734674225316925, 'recall': 0.14184766270364124, 'fmeasure': 0.12878645462734362}, 'rougeLsum': {'precision': 0.12729427292120832, 'recall': 0.14218158552397694, 'fmeasure': 0.12885735050331099}}}\n",
      "\n",
      "\n",
      "# Some test phrases generated using google/pegasus-large:\n",
      "Argument: We should legalize polygamy polygamy, if legalized, could cause many issues with marital benefits, such as health insurance. \n",
      "Generated key-point: Affirmative action is necessary to compensate for the harms that have been done to the family\n",
      "\n",
      "\n",
      "Argument: We should subsidize stay-at-home dads When dads stay at home, they are being supported by a working mom who gets paid less than men, so the father should be subsidized to make up the difference. \n",
      "Generated key-point: offer a better compensate for the child's education\n",
      "\n",
      "\n",
      "Argument: We should ban algorithmic trading sometimes in a company the supply exceeds the demand so this process would work well. \n",
      "Generated key-point: is is necessary to reduce the risk of inserting bias/harming the market\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr_metrics = []; val_metrics = [];\n",
    "save_res = \"\"\n",
    "for i in range(5):\n",
    "    if i == 4:\n",
    "        save_res = 'pegasus_large_val.csv'\n",
    "    tr, val = train_best_model(config, df_train, df_val, df_test, 100, None, ['rouge'], device, save_res)\n",
    "    tr_metrics.append(tr)\n",
    "    val_metrics.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24be24cc-cac7-47b1-a4f8-431e6605b701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Mean: 0.18497260165827967 and Variance: 2.05267845038774e-05\n",
      "Validation Mean: 0.14080026448339333 and Variance: 0.0002468332494546151\n"
     ]
    }
   ],
   "source": [
    "set_name = \"Train\"; compute_mean_var(tr_metrics, set_name)\n",
    "set_name = \"Validation\"; compute_mean_var(val_metrics, set_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25e63b9-345c-4b96-8ada-943b5e979265",
   "metadata": {},
   "source": [
    "# Get grid results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7eb13f2e-9964-4dc8-a029-b98b45a719a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv('task2_grid_results.csv', sep='#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b250462-dd7e-4215-a012-fc2255f77285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 92 entries, 0 to 91\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   max_length          92 non-null     int64  \n",
      " 1   loss                0 non-null      float64\n",
      " 2   batch_size          92 non-null     int64  \n",
      " 3   optimizer           92 non-null     object \n",
      " 4   lr                  92 non-null     float64\n",
      " 5   eps                 92 non-null     float64\n",
      " 6   epochs              92 non-null     int64  \n",
      " 7   warmup_steps        92 non-null     float64\n",
      " 8   weight_decay        92 non-null     float64\n",
      " 9   mode                0 non-null      float64\n",
      " 10  match_model_type    0 non-null      float64\n",
      " 11  model_type          92 non-null     object \n",
      " 12  train_metrics       92 non-null     object \n",
      " 13  validation_metrics  92 non-null     object \n",
      "dtypes: float64(7), int64(3), object(4)\n",
      "memory usage: 10.2+ KB\n"
     ]
    }
   ],
   "source": [
    "results.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "375dfb68-9147-4263-9aa7-5a48355e8b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [{'rouge': {'rouge1': {'precision': 0.10039630...\n",
       "1     [{'rouge': {'rouge1': {'precision': 0.08707772...\n",
       "2     [{'rouge': {'rouge1': {'precision': 0.08014984...\n",
       "3     [{'rouge': {'rouge1': {'precision': 0.10061315...\n",
       "4     [{'rouge': {'rouge1': {'precision': 0.23631677...\n",
       "                            ...                        \n",
       "87    [{'rouge': {'rouge1': {'precision': 0.15894416...\n",
       "88    [{'rouge': {'rouge1': {'precision': 0.20541208...\n",
       "89    [{'rouge': {'rouge1': {'precision': 0.17061319...\n",
       "90    [{'rouge': {'rouge1': {'precision': 0.14575034...\n",
       "91    [{'rouge': {'rouge1': {'precision': 0.11104206...\n",
       "Name: train_metrics, Length: 92, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.train_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b6ff14df-2ea0-40fb-9ca2-aadb8a058fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(results.train_metrics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "34eb4f6c-b1d9-43ac-a481-118c0b43999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best10 = []\n",
    "idxbest = []\n",
    "\n",
    "for i, elem in enumerate(results.validation_metrics):\n",
    "    res = eval(elem)\n",
    "    #prendere fmeasure di rouge1\n",
    "    res = res['rouge']['rouge1']['fmeasure']\n",
    "    if len(best10) >= 10:\n",
    "        minimum = min(best10)\n",
    "        if res > minimum:\n",
    "            index = best10.index(minimum)\n",
    "            best10[index] = res\n",
    "            idxbest[index] = i\n",
    "    else:\n",
    "        best10.append(res)\n",
    "        idxbest.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7bb6bf59-041b-474b-85d0-8c21a9cd9f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.15879839500469128,\n",
       " 0.15012853030506526,\n",
       " 0.15022028537844268,\n",
       " 0.15718535078639642,\n",
       " 0.15010203139597608,\n",
       " 0.1521299572213119,\n",
       " 0.15018620742160183,\n",
       " 0.15038552511804282,\n",
       " 0.15860736272116935,\n",
       " 0.15381080565610264]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "63fc9c0b-fab1-454f-ae02-b3e22b4d4751",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestRes = results.iloc[idxbest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "42de2a99-26aa-4a65-8866-d7cca315ce16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "[{'rouge': {'rouge1': {'precision': 0.20536540229204026, 'recall': 0.30397059020633366, 'fmeasure': 0.22960817397821273}, 'rouge2': {'precision': 0.11689241352290375, 'recall': 0.1903683437848577, 'fmeasure': 0.13765592356711376}, 'rougeL': {'precision': 0.19918519435811566, 'recall': 0.29428457002736663, 'fmeasure': 0.22242326564280956}, 'rougeLsum': {'precision': 0.1989722223502496, 'recall': 0.2939702841676124, 'fmeasure': 0.22218544841579302}}}, {'rouge': {'rouge1': {'precision': 0.30693650339707285, 'recall': 0.4590588903726647, 'fmeasure': 0.3520106588761448}, 'rouge2': {'precision': 0.21569490274317776, 'recall': 0.3373626879695487, 'fmeasure': 0.25003386942665773}, 'rougeL': {'precision': 0.29887467885203717, 'recall': 0.4469930796161278, 'fmeasure': 0.34291551605608594}, 'rougeLsum': {'precision': 0.2987769796873577, 'recall': 0.4467376835849217, 'fmeasure': 0.3426727485780711}}}]\n",
      "Val\n",
      "{'rouge': {'rouge1': {'precision': 0.14315613713391884, 'recall': 0.19004528331974024, 'fmeasure': 0.15879839500469128}, 'rouge2': {'precision': 0.031683144251079, 'recall': 0.04124468023924542, 'fmeasure': 0.034597199658356476}, 'rougeL': {'precision': 0.1316994348983643, 'recall': 0.17521648289311376, 'fmeasure': 0.14599660191866132}, 'rougeLsum': {'precision': 0.131818304170846, 'recall': 0.17537702333898025, 'fmeasure': 0.14618883555667816}}}\n",
      "google/pegasus-large max_l: 100 batch:8 lr: 0.0003 opt:adamW eps: 1e-08 epochs:2 wd: 0.0 warmup:100.0\n",
      "\n",
      "\n",
      "Train\n",
      "[{'rouge': {'rouge1': {'precision': 0.10092266440973773, 'recall': 0.23506809719669833, 'fmeasure': 0.1350952553065328}, 'rouge2': {'precision': 0.02621402273373382, 'recall': 0.06615324487385071, 'fmeasure': 0.03565533334594297}, 'rougeL': {'precision': 0.09243594479518444, 'recall': 0.21707407363544762, 'fmeasure': 0.12381288015933221}, 'rougeLsum': {'precision': 0.09227494801143249, 'recall': 0.21699291618634275, 'fmeasure': 0.12370204329677244}}}]\n",
      "Val\n",
      "{'rouge': {'rouge1': {'precision': 0.11139972951234187, 'recall': 0.246374687611101, 'fmeasure': 0.15012853030506526}, 'rouge2': {'precision': 0.03600505123842716, 'recall': 0.07385157867494821, 'fmeasure': 0.04767181271434576}, 'rougeL': {'precision': 0.10498100600202631, 'recall': 0.23248148540268143, 'fmeasure': 0.14148741389911051}, 'rougeLsum': {'precision': 0.10498161754355854, 'recall': 0.23223739072923882, 'fmeasure': 0.14136977763524847}}}\n",
      "google/pegasus-large max_l: 100 batch:8 lr: 1e-08 opt:adamW eps: 1e-08 epochs:1 wd: 1e-08 warmup:100.0\n",
      "\n",
      "\n",
      "Train\n",
      "[{'rouge': {'rouge1': {'precision': 0.1003740991572063, 'recall': 0.23466469130109502, 'fmeasure': 0.13474827925961919}, 'rouge2': {'precision': 0.02644005794648432, 'recall': 0.066658794083146, 'fmeasure': 0.03588321119005927}, 'rougeL': {'precision': 0.09171975094844118, 'recall': 0.2163884035132524, 'fmeasure': 0.12319809989463049}, 'rougeLsum': {'precision': 0.09165400878290633, 'recall': 0.21631456098367782, 'fmeasure': 0.12318855707028814}}}]\n",
      "Val\n",
      "{'rouge': {'rouge1': {'precision': 0.11140139750074252, 'recall': 0.24611294753957835, 'fmeasure': 0.15022028537844268}, 'rouge2': {'precision': 0.03607296813019316, 'recall': 0.07396911663216008, 'fmeasure': 0.047743984857767006}, 'rougeL': {'precision': 0.10480434871619385, 'recall': 0.23231538992408587, 'fmeasure': 0.14119066448871478}, 'rougeLsum': {'precision': 0.10509675650140848, 'recall': 0.23257902524206908, 'fmeasure': 0.14147650493531555}}}\n",
      "google/pegasus-large max_l: 100 batch:8 lr: 1e-07 opt:adamW eps: 1e-08 epochs:1 wd: 1e-08 warmup:100.0\n",
      "\n",
      "\n",
      "Train\n",
      "[{'rouge': {'rouge1': {'precision': 0.09944746741523788, 'recall': 0.2205635693893493, 'fmeasure': 0.12925660650548187}, 'rouge2': {'precision': 0.02579364836046947, 'recall': 0.06232960844128832, 'fmeasure': 0.03438479961015012}, 'rougeL': {'precision': 0.09132270621460613, 'recall': 0.2040088733628648, 'fmeasure': 0.11873515116249869}, 'rougeLsum': {'precision': 0.09120357366213652, 'recall': 0.2038660828689333, 'fmeasure': 0.11860343274299284}}}]\n",
      "Val\n",
      "{'rouge': {'rouge1': {'precision': 0.14553440315308983, 'recall': 0.20988005050505087, 'fmeasure': 0.15718535078639642}, 'rouge2': {'precision': 0.06116253589878909, 'recall': 0.06255283816425115, 'fmeasure': 0.05574390249363753}, 'rougeL': {'precision': 0.14089220683290432, 'recall': 0.20154269794215474, 'fmeasure': 0.1514229865283594}, 'rougeLsum': {'precision': 0.14057761551167952, 'recall': 0.2013043216847568, 'fmeasure': 0.15128216885146664}}}\n",
      "google/pegasus-large max_l: 100 batch:8 lr: 1e-05 opt:adamW eps: 1e-08 epochs:1 wd: 1e-08 warmup:1000.0\n",
      "\n",
      "\n",
      "Train\n",
      "[{'rouge': {'rouge1': {'precision': 0.10123071528765684, 'recall': 0.23696773248093783, 'fmeasure': 0.13593579308407522}, 'rouge2': {'precision': 0.026602884849786852, 'recall': 0.06733414923823422, 'fmeasure': 0.036217495879017694}, 'rougeL': {'precision': 0.09235515097049989, 'recall': 0.21838332694049906, 'fmeasure': 0.12420322791949649}, 'rougeLsum': {'precision': 0.0924881500258659, 'recall': 0.21850778648122454, 'fmeasure': 0.12439942021111942}}}]\n",
      "Val\n",
      "{'rouge': {'rouge1': {'precision': 0.11142817935297201, 'recall': 0.24638171309367005, 'fmeasure': 0.15010203139597608}, 'rouge2': {'precision': 0.0359458367630522, 'recall': 0.0733645330112721, 'fmeasure': 0.047446399761820225}, 'rougeL': {'precision': 0.10501492053402273, 'recall': 0.2323721166112473, 'fmeasure': 0.1413507656758014}, 'rougeLsum': {'precision': 0.10500046140631222, 'recall': 0.23227715822824546, 'fmeasure': 0.14141313790079552}}}\n",
      "google/pegasus-large max_l: 100 batch:8 lr: 1e-09 opt:adamW eps: 1e-08 epochs:1 wd: 1e-08 warmup:100.0\n",
      "\n",
      "\n",
      "Train\n",
      "[{'rouge': {'rouge1': {'precision': 0.24970041459872655, 'recall': 0.34578087312180894, 'fmeasure': 0.27273373020309094}, 'rouge2': {'precision': 0.15850273551795352, 'recall': 0.23528151682016163, 'fmeasure': 0.18022282327181377}, 'rougeL': {'precision': 0.243253590188173, 'recall': 0.33636514490355973, 'fmeasure': 0.2657032524112488}, 'rougeLsum': {'precision': 0.24320789641785642, 'recall': 0.3360261255406807, 'fmeasure': 0.2653632080147027}}}, {'rouge': {'rouge1': {'precision': 0.33873326680773097, 'recall': 0.4822870752530113, 'fmeasure': 0.38306463684558834}, 'rouge2': {'precision': 0.2516445846096821, 'recall': 0.3754604612766993, 'fmeasure': 0.288322758306934}, 'rougeL': {'precision': 0.332712262957523, 'recall': 0.4732890372821339, 'fmeasure': 0.3760628500341451}, 'rougeLsum': {'precision': 0.3326083544930637, 'recall': 0.473153695099973, 'fmeasure': 0.37594780519020643}}}]\n",
      "Val\n",
      "{'rouge': {'rouge1': {'precision': 0.14555163018638106, 'recall': 0.17374783026956991, 'fmeasure': 0.1521299572213119}, 'rouge2': {'precision': 0.02172182222733693, 'recall': 0.026977297561536676, 'fmeasure': 0.022926351393179747}, 'rougeL': {'precision': 0.12696773276217027, 'recall': 0.15511500878348716, 'fmeasure': 0.13390120263685862}, 'rougeLsum': {'precision': 0.12648952953660464, 'recall': 0.15473609019804707, 'fmeasure': 0.13352536642291063}}}\n",
      "google/pegasus-large max_l: 100 batch:8 lr: 0.0005 opt:adamW eps: 1e-08 epochs:2 wd: 1e-08 warmup:100.0\n",
      "\n",
      "\n",
      "Train\n",
      "[{'rouge': {'rouge1': {'precision': 0.10046929526948198, 'recall': 0.23533627681286678, 'fmeasure': 0.13469161739435515}, 'rouge2': {'precision': 0.025965286296036536, 'recall': 0.0658719299718345, 'fmeasure': 0.03536110382912215}, 'rougeL': {'precision': 0.09177848253638646, 'recall': 0.216901982515727, 'fmeasure': 0.12319920748936235}, 'rougeLsum': {'precision': 0.09181734877983073, 'recall': 0.2169898204036848, 'fmeasure': 0.12324925934297903}}}]\n",
      "Val\n",
      "{'rouge': {'rouge1': {'precision': 0.1114530833320799, 'recall': 0.24607566346696802, 'fmeasure': 0.15018620742160183}, 'rouge2': {'precision': 0.0361430163240425, 'recall': 0.07392202956061651, 'fmeasure': 0.047814992104511345}, 'rougeL': {'precision': 0.10470495022525123, 'recall': 0.23175639024823835, 'fmeasure': 0.14084869357558483}, 'rougeLsum': {'precision': 0.10540230111841409, 'recall': 0.23287262506012527, 'fmeasure': 0.14189097494946196}}}\n",
      "google/pegasus-large max_l: 100 batch:8 lr: 1e-09 opt:adamW eps: 1e-08 epochs:1 wd: 1e-08 warmup:1000.0\n",
      "\n",
      "\n",
      "Train\n",
      "[{'rouge': {'rouge1': {'precision': 0.10082901002010408, 'recall': 0.23620915246440388, 'fmeasure': 0.1354008825296722}, 'rouge2': {'precision': 0.026240939192664247, 'recall': 0.06691542220238977, 'fmeasure': 0.03590581903174446}, 'rougeL': {'precision': 0.09205025765149313, 'recall': 0.21797680097124772, 'fmeasure': 0.12385199421056524}, 'rougeLsum': {'precision': 0.0921244054993749, 'recall': 0.21799969682622627, 'fmeasure': 0.12394940589195355}}}]\n",
      "Val\n",
      "{'rouge': {'rouge1': {'precision': 0.11169161543202513, 'recall': 0.2471767689838345, 'fmeasure': 0.15038552511804282}, 'rouge2': {'precision': 0.03608816721428609, 'recall': 0.07390801127214168, 'fmeasure': 0.04765742270414271}, 'rougeL': {'precision': 0.10501447148865668, 'recall': 0.23249403324131618, 'fmeasure': 0.14138217812735449}, 'rougeLsum': {'precision': 0.10487178795203131, 'recall': 0.23192771399293172, 'fmeasure': 0.1411937616374398}}}\n",
      "google/pegasus-large max_l: 100 batch:8 lr: 1e-07 opt:adamW eps: 1e-08 epochs:1 wd: 1e-08 warmup:1000.0\n",
      "\n",
      "\n",
      "Train\n",
      "[{'rouge': {'rouge1': {'precision': 0.21626437116736105, 'recall': 0.3033474623060451, 'fmeasure': 0.23359372057908684}, 'rouge2': {'precision': 0.11989021821241459, 'recall': 0.18716387204885554, 'fmeasure': 0.13722812876967008}, 'rougeL': {'precision': 0.20920648300357478, 'recall': 0.2926966717145284, 'fmeasure': 0.22547284253899064}, 'rougeLsum': {'precision': 0.20901793702796423, 'recall': 0.2927885673083745, 'fmeasure': 0.22552219965935819}}}]\n",
      "Val\n",
      "{'rouge': {'rouge1': {'precision': 0.15551993045199594, 'recall': 0.18104697989208884, 'fmeasure': 0.15860736272116935}, 'rouge2': {'precision': 0.04128917580004536, 'recall': 0.04168140384172991, 'fmeasure': 0.039060644466155356}, 'rougeL': {'precision': 0.14390288693557585, 'recall': 0.1642714345839349, 'fmeasure': 0.14500781682603872}, 'rougeLsum': {'precision': 0.1431876337407031, 'recall': 0.16414968133718166, 'fmeasure': 0.1447563492261198}}}\n",
      "google/pegasus-large max_l: 100 batch:8 lr: 0.0003 opt:adamW eps: 1e-08 epochs:1 wd: 0.0 warmup:100.0\n",
      "\n",
      "\n",
      "Train\n",
      "[{'rouge': {'rouge1': {'precision': 0.10169606391984531, 'recall': 0.22488482239382537, 'fmeasure': 0.13222920213735456}, 'rouge2': {'precision': 0.02726463311454688, 'recall': 0.06387856537298657, 'fmeasure': 0.03567679160568133}, 'rougeL': {'precision': 0.09349501076478114, 'recall': 0.2079024118527414, 'fmeasure': 0.12154346533810909}, 'rougeLsum': {'precision': 0.09353974836607, 'recall': 0.20774657779084477, 'fmeasure': 0.1215560223275246}}}]\n",
      "Val\n",
      "{'rouge': {'rouge1': {'precision': 0.12855894128072898, 'recall': 0.22808784663404258, 'fmeasure': 0.15381080565610264}, 'rouge2': {'precision': 0.049314870622415366, 'recall': 0.06583850931677014, 'fmeasure': 0.051253176420588534}, 'rougeL': {'precision': 0.12292426783515399, 'recall': 0.21664245325930137, 'fmeasure': 0.14625703285517222}, 'rougeLsum': {'precision': 0.12299854447792477, 'recall': 0.21689772335152796, 'fmeasure': 0.14658837256232476}}}\n",
      "google/pegasus-large max_l: 100 batch:8 lr: 1e-05 opt:adamW eps: 1e-08 epochs:1 wd: 0.0 warmup:1000.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for i in range(i, len(bestRes)):\n",
    "    print(\"Train\")\n",
    "    print(bestRes.iloc[i]['train_metrics'])\n",
    "    print(\"Val\")\n",
    "    print(bestRes.iloc[i]['validation_metrics'])\n",
    "    print(f\"{bestRes.iloc[i]['model_type']} max_l: {bestRes.iloc[i]['max_length']} batch:{bestRes.iloc[i]['batch_size']} lr: {bestRes.iloc[i]['lr']} opt:{bestRes.iloc[i]['optimizer']} eps: {bestRes.iloc[i]['eps']} epochs:{bestRes.iloc[i]['epochs']} wd: {bestRes.iloc[i]['weight_decay']} warmup:{bestRes.iloc[i]['warmup_steps']}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
